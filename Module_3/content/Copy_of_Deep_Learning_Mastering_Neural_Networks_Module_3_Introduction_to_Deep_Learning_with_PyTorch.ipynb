{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjYDWVa3V4Tu"
      },
      "source": [
        "# **Module 3: Introduction to Deep Learning with PyTorch**\n",
        "\n",
        "## Introduction\n",
        "In previous notebooks, we have created, trained, and evaluated neural networks using python classes we have developed. There are many open source libraries that help optimize and make the process of creating these models repeatable, and we will look at one of the most popular ones in this notebook: PyTorch.\n",
        "\n",
        "Now the we know the basics of using tensors in [Pytorch](https://pytorch.org/) from the previous notebook, we will use them to create Deep Learning models. In the following notebook we will go through an example of creating both a classification and regression model in Pytorch on a large publicly available dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88DD4RKAU4J-"
      },
      "source": [
        "## Classification with PyTorch\n",
        "\n",
        "In this first example, we will be using PyTorch to perform a binary classification task on an open source dataset. The dataset in particular is the Abalone Age Dataset and can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/).\n",
        "\n",
        "[Abalone](https://en.wikipedia.org/wiki/Abalone) are a type of marine snail, and their age can be determined by performing an extremely manual task of counting rings inside of their shell. This dataset contains a number of measurements that can be used to predict the number of rings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIOAvcyPl-Hd"
      },
      "source": [
        "### Datset Download and Module Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwGngEegU4KB"
      },
      "outputs": [],
      "source": [
        "# Download the abalone .csv files from data archive if needed\n",
        "!rm abalone.data abalone.names\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S982Co0hU4KB"
      },
      "outputs": [],
      "source": [
        "# As you can see we now require a lot of different modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time, copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZuTMIgTU4KE"
      },
      "source": [
        "### Prepare the abalone data for classification\n",
        "\n",
        "We want to create a classification problem to differentiate old and young abalones. We designate any abalone with less than 10 rings as young, and all others as old."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "qR67MHXsU4KE",
        "outputId": "103d9710-8571-4ba8-c9cc-3e32b406b42e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Length', 'Diameter', 'Height', 'Whole weight']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
              "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
              "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
              "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
              "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
              "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
              "\n",
              "   Shell weights  Rings  Old  \n",
              "0          0.150     15    1  \n",
              "1          0.070      7    0  \n",
              "2          0.210      9    0  \n",
              "3          0.155     10    1  \n",
              "4          0.055      7    0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dabd45c3-a164-4be0-a864-0a5129abc591\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>Whole weight</th>\n",
              "      <th>Shucked weight</th>\n",
              "      <th>Viscera weight</th>\n",
              "      <th>Shell weights</th>\n",
              "      <th>Rings</th>\n",
              "      <th>Old</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dabd45c3-a164-4be0-a864-0a5129abc591')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dabd45c3-a164-4be0-a864-0a5129abc591 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dabd45c3-a164-4be0-a864-0a5129abc591');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-81ee1816-e6e9-4286-bb93-926472c9e522\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-81ee1816-e6e9-4286-bb93-926472c9e522')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-81ee1816-e6e9-4286-bb93-926472c9e522 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4177,\n  \"fields\": [\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"M\",\n          \"F\",\n          \"I\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12009291256479998,\n        \"min\": 0.075,\n        \"max\": 0.815,\n        \"num_unique_values\": 134,\n        \"samples\": [\n          0.815,\n          0.65,\n          0.29\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Diameter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09923986613365918,\n        \"min\": 0.055,\n        \"max\": 0.65,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          0.09,\n          0.35,\n          0.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04182705660725703,\n        \"min\": 0.0,\n        \"max\": 1.13,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.235,\n          0.035,\n          0.015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Whole weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49038901823099723,\n        \"min\": 0.002,\n        \"max\": 2.8255,\n        \"num_unique_values\": 2429,\n        \"samples\": [\n          1.2825,\n          1.09,\n          0.131\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Shucked weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22196294903322014,\n        \"min\": 0.001,\n        \"max\": 1.488,\n        \"num_unique_values\": 1515,\n        \"samples\": [\n          0.2105,\n          0.0645,\n          0.476\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Viscera weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10961425025968442,\n        \"min\": 0.0005,\n        \"max\": 0.76,\n        \"num_unique_values\": 880,\n        \"samples\": [\n          0.0645,\n          0.0095,\n          0.1115\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Shell weights\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13920266952238575,\n        \"min\": 0.0015,\n        \"max\": 1.005,\n        \"num_unique_values\": 926,\n        \"samples\": [\n          0.3745,\n          0.2825,\n          0.49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rings\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 29,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          11,\n          27,\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Old\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "column_names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole weight\",\n",
        "                \"Shucked weight\", \"Viscera weight\", \"Shell weights\", \"Rings\"]\n",
        "df = pd.read_csv('abalone.data', header=None, names=column_names)\n",
        "df['Old'] = 0  # By default, abalone is Young\n",
        "df.loc[(df['Rings'] >= 10), 'Old'] = 1 # 10 rings or more means an Abalone is old\n",
        "class_labels = ['Young', 'Old']   # [0, 1], [N, P]\n",
        "numerical_feature_columns = column_names[1:5] # We first only want to classify with numerical features excluding sex and only the whole weight\n",
        "print(numerical_feature_columns)\n",
        "label_column = 'Old'\n",
        "\n",
        "\n",
        "df.head() # Visualize a subset of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ttQznume2S"
      },
      "source": [
        "### Train-Validation-Test Split and Standardization\n",
        "\n",
        "For this classification task, we will split our dataset into 3 different subsets as described earlier in the module. The train subset will be the largest and this is the subset is used to train and optimize the weights of the model. Next is the validation subset, which will be used as an unbiased evaluation of the model and help to tune different hyperparameters. Lastly is the test subset, which is used to perform an unbiased evaluation of the fully trained model.\n",
        "\n",
        "Additionally, we would like to perform data standardization by removing the mean and scaling by the variance. We will perform this on each of the features however we want to ensure that this standardization function is **computed** on the **train data subset**. We then **apply** the standardization function to the **validation and test datasets**. By ensuring that the standardization is **only computed** on the train subset, we gaurantee that no information from the validation or test datasets is leaking into the train subset.\n",
        "\n",
        "Below we provide a function for splitting a given dataset into a 60-20-20 train-validation-test and performing data standardization using the sklearn StandardScaler class for PyTorch consumption:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def train_test_val_split(df, feature_columns, label_column, ct):\n",
        "  # First perform an 80/20 train/test split\n",
        "  initial_train_split = df.sample(frac=.8,random_state=42)\n",
        "  test = df.drop(initial_train_split.index)\n",
        "\n",
        "  # Next perform a 75/25 train/val split\n",
        "  train = initial_train_split.sample(frac=.75, random_state=42)\n",
        "  val = initial_train_split.drop(train.index)\n",
        "\n",
        "  # Split all of the datasets into features and labels (x and y)\n",
        "  train_x = train[feature_columns]\n",
        "  # We won't be transforming the labels so they can go straight to torch tensors\n",
        "  train_y = torch.from_numpy(train[label_column].values)\n",
        "\n",
        "  val_x = val[feature_columns]\n",
        "  val_y = torch.from_numpy(val[label_column].values)\n",
        "\n",
        "  test_x = test[feature_columns]\n",
        "  test_y = torch.from_numpy(test[label_column].values)\n",
        "\n",
        "  # Fit our ColumnTransformer to the train dataset\n",
        "  # 仅在训练集上拟合标准化器\n",
        "  ct.fit(train_x)\n",
        "\n",
        "  # Perform the standardization on each of the x datasets\n",
        "  # 对所有数据集应用标准化（使用训练集的统计量）\n",
        "  train_x = ct.transform(train_x)\n",
        "  val_x = ct.transform(val_x)\n",
        "  test_x = ct.transform(test_x)\n",
        "\n",
        "  # Transform our datasets into Torch Tensors\n",
        "  train_x = torch.from_numpy(train_x).float()\n",
        "  val_x = torch.from_numpy(val_x).float()\n",
        "  test_x = torch.from_numpy(test_x).float()\n",
        "\n",
        "  # Create the input-label pair datasets to be consumed by PyTorch\n",
        "  train_dataset = TensorDataset(train_x, train_y)\n",
        "  test_dataset = TensorDataset(test_x, test_y)\n",
        "  val_dataset = TensorDataset(val_x, val_y)\n",
        "  return train_dataset, test_dataset, val_dataset\n"
      ],
      "metadata": {
        "id": "T_8oUtiUA3Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our StandardScaler to use for datastandardization\n",
        "# More here: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer\n",
        "ct = ColumnTransformer([('numerical_features', StandardScaler(), numerical_feature_columns)], remainder='passthrough')\n",
        "\n",
        "train_dataset, test_dataset, val_dataset = train_test_val_split(df, numerical_feature_columns, label_column, ct)"
      ],
      "metadata": {
        "id": "lVZXumJh2BVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90DlOd1UqgEg"
      },
      "source": [
        "### Model Hyperparameters and Definition\n",
        "Now that we can easily split our dataset into train, validation, and test subsets, we turn to the selection of our hyperparameters. These hyperparameters include model architecture decisions (number of hidden layers) and external training parameters such as batch size, learning rate, and number of training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyTKvqUAU4KG"
      },
      "outputs": [],
      "source": [
        "# device config (train our model on GPU if it is available which is much faster)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'device={device}')\n",
        "# hyperparameters\n",
        "\n",
        "# model architecture\n",
        "input_size = len(numerical_feature_columns) # Make sure to remove the index column\n",
        "print(f'input size={input_size}')\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 64\n",
        "hidden_size3 = 64\n",
        "num_classes = 2\n",
        "\n",
        "# external training parameters\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Selection\n",
        "\n",
        "In this walkthrough, we will use our validation set to determine two different hyperparameters.\n",
        "\n",
        "The first is the total number of Epochs to train for. At the end of our entire training process we will select the weights from the epoch with the highest validation accuracy.\n",
        "\n",
        "The second is number of layers - we will try both a 2-hidden-layer and 3-hidden-layer network. Again, which ever has the best performance on the validation set will be our final model of choice."
      ],
      "metadata": {
        "id": "MxCCd0idiKbp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSjFybbmU4KH"
      },
      "outputs": [],
      "source": [
        "# Simple two-hidden-layer classification model\n",
        "class SimpleClassifier2Layer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
        "        super(SimpleClassifier2Layer, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size2, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Simple three-hidden-layer classification model\n",
        "class SimpleClassifier3Layer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):\n",
        "        super(SimpleClassifier3Layer, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size2, hidden_size3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size3, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o51QXeXxU4KH"
      },
      "outputs": [],
      "source": [
        "two_layer_model = SimpleClassifier2Layer(input_size, hidden_size1, hidden_size2, num_classes).to(device)\n",
        "print(two_layer_model)\n",
        "\n",
        "three_layer_model = SimpleClassifier3Layer(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes).to(device)\n",
        "print(three_layer_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNoWdg86vq-N"
      },
      "source": [
        "### Final Data Preparation\n",
        "\n",
        "Now that we have our model and our hyperparameters defined, we can do the last data preparation step. We will use a built in PyTorch class called a DataLoader with our batch size specified earlier. This DataLoader will be responsible for feeding in the batches of data to the model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js3SG4MwoZ8Z"
      },
      "outputs": [],
      "source": [
        "dataloaders = {'train': DataLoader(train_dataset, batch_size=batch_size),\n",
        "               'val': DataLoader(val_dataset, batch_size=batch_size),\n",
        "               'test': DataLoader(test_dataset, batch_size=batch_size)}\n",
        "\n",
        "dataset_sizes = {'train': len(train_dataset),\n",
        "                 'val': len(val_dataset),\n",
        "                 'test': len(test_dataset)}\n",
        "print(f'dataset_sizes = {dataset_sizes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFLV8A-IwJeN"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "Now that we have our model architecture defined and our data ready for consumption, we must define the training function. This function will be responsible for updating all of the weights of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5da9eptsU4KH"
      },
      "outputs": [],
      "source": [
        "# From https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "\n",
        "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) # keep the best weights stored separately\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Each epoch has a training, validation, and test phase\n",
        "    phases = ['train', 'val', 'test']\n",
        "\n",
        "    # Keep track of how loss and accuracy evolves during training\n",
        "    training_curves = {}\n",
        "    for phase in phases:\n",
        "        training_curves[phase+'_loss'] = []\n",
        "        training_curves[phase+'_acc'] = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in phases:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                # This ensures all of our datapoints are flattened\n",
        "                # before feeding them to our model\n",
        "                inputs = inputs.view(inputs.shape[0],-1)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    # 对模型输出提取预测类别（最大概率对应的类别索引），用于计算准确率。\n",
        "                    _, predictions = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + update weights only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(predictions == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            training_curves[phase+'_loss'].append(epoch_loss)\n",
        "            training_curves[phase+'_acc'].append(epoch_acc)\n",
        "\n",
        "            print(f'{phase:5} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # 基于验证集准确率最高保存模型\n",
        "            # deep copy the model if it's the best accuracy (based on validation)\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_epoch = epoch\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f} at epoch {best_epoch}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, training_curves"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have written our training function above to be architecture independent. That means we can use the same function to train both the two-hidden-layer and three-hidden-layer networks."
      ],
      "metadata": {
        "id": "LmhLnJZuh7kj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhu-dTNRU4KJ"
      },
      "outputs": [],
      "source": [
        "# Two-hidden-Layer Training\n",
        "# loss and optimizer\n",
        "# 使用交叉熵损失 衡量预测类别分布与真实标签的差异。\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification!\n",
        "# optimizer 负责根据当前学习率更新模型参数\n",
        "optimizer = torch.optim.Adam(two_layer_model.parameters(), lr=learning_rate)\n",
        "# scheduler 负责调整 optimizer 中的学习率\n",
        "# scheduler 需要访问并修改 optimizer 中的学习率，因此要传入参数optimizer\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "# Train the model. We also will store the results of training to visualize\n",
        "two_layer_model, training_curves_two_layer = train_model(two_layer_model, dataloaders, dataset_sizes,\n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz95CvYx73p5"
      },
      "source": [
        "In order to determine which weight we would like to use for our final model, we will look for the Epoch that performed best on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Three-hidden-Layer Training\n",
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification!\n",
        "optimizer = torch.optim.Adam(three_layer_model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "# Train the model. We also will store the results of training to visualize\n",
        "three_layer_model, training_curves_three_layer = train_model(three_layer_model, dataloaders, dataset_sizes,\n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "WTzVkL72jTLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After our training, we saw comparable accuracy between our two different models so we will use the three-layer model going forward but will explore other methods of improving our performance."
      ],
      "metadata": {
        "id": "OGRiX8hgkE_0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1vxiEA_zX-B"
      },
      "source": [
        "### Training Curves and Metrics\n",
        "\n",
        "Below we have provided functions for visualizing the training curves and metrics for the classification problem. Feel free to use these functions in the future!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DDWNUtOU4KI"
      },
      "outputs": [],
      "source": [
        "def plot_training_curves(training_curves,\n",
        "                         phases=['train', 'val', 'test'],\n",
        "                         metrics=['loss','acc']):\n",
        "    epochs = list(range(len(training_curves['train_loss'])))\n",
        "    for metric in metrics:\n",
        "        plt.figure()\n",
        "        plt.title(f'Training curves - {metric}')\n",
        "        for phase in phases:\n",
        "            key = phase+'_'+metric\n",
        "            if key in training_curves:\n",
        "                plt.plot(epochs, training_curves[phase+'_'+metric])\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(labels=phases)\n",
        "\n",
        "def classify_predictions(model, device, dataloader):\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "    all_labels = torch.tensor([]).to(device)\n",
        "    all_scores = torch.tensor([]).to(device)\n",
        "    all_preds = torch.tensor([]).to(device)\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = torch.softmax(model(inputs),dim=1)\n",
        "        # 对模型输出提取预测类别（最大概率对应的类别索引），用于计算准确率。\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        scores = outputs[:,1]\n",
        "        all_labels = torch.cat((all_labels, labels), 0)\n",
        "        all_scores = torch.cat((all_scores, scores), 0)\n",
        "        all_preds = torch.cat((all_preds, preds), 0)\n",
        "    return all_preds.detach().cpu(), all_labels.detach().cpu(), all_scores.detach().cpu()\n",
        "\n",
        "def plot_regression(model, device, dataloader):\n",
        "    preds, targets = regress_predictions(model, device, dataloader)\n",
        "    plt.figure()\n",
        "    plt.title(f'Regression results')\n",
        "    plt.scatter(targets, preds)\n",
        "    xmin = min(min(targets), min(preds))\n",
        "    xmax = max(max(targets), max(targets))\n",
        "    plt.plot([xmin, xmax], [xmin, xmax], 'black', linestyle='--', marker='')\n",
        "    plt.xlabel('target value')\n",
        "    plt.ylabel('prediced value')\n",
        "\n",
        "def regress_predictions(model, device, dataloader):\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "    all_targets = torch.tensor([]).to(device)\n",
        "    all_preds = torch.tensor([]).to(device)\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        preds = model(inputs)\n",
        "        all_targets = torch.cat((all_targets, targets), 0)\n",
        "        all_preds = torch.cat((all_preds, preds), 0)\n",
        "    return all_preds.detach().cpu(), all_targets.detach().cpu()\n",
        "\n",
        "def plot_metrics(model, device, dataloaders, phase='test'):\n",
        "    preds, labels, scores = classify_predictions(model, device, dataloaders[phase])\n",
        "\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(labels, scores)\n",
        "    auc = metrics.roc_auc_score(labels, preds)\n",
        "\n",
        "    disp = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc)\n",
        "    ind = np.argmin(np.abs(thresholds - 0.5))\n",
        "    ax = disp.plot().ax_\n",
        "    ax.scatter(fpr[ind], tpr[ind], color = 'red')\n",
        "    ax.set_title('ROC Curve (red dot at threshold = 0.5)')\n",
        "\n",
        "    cm = metrics.confusion_matrix(labels, preds)\n",
        "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "    #disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    ax = disp.plot().ax_\n",
        "    ax.set_title('Confusion Matrix -- counts')\n",
        "\n",
        "    ncm = metrics.confusion_matrix(labels, preds, normalize='true')\n",
        "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=ncm)\n",
        "    ax = disp.plot().ax_\n",
        "    ax.set_title('Confusion Matrix -- rates')\n",
        "\n",
        "    TN, FP, FN, TP = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "    N, P = TN + FP, TP + FN\n",
        "    ACC = (TP + TN)/(P+N)\n",
        "    TPR, FPR, FNR, TNR = TP/P, FP/N, FN/P, TN/N\n",
        "    print(f'\\nAt default threshold:')\n",
        "    print(f' TN = {TN:5},  FP = {FP:5} -> N = {N:5}')\n",
        "    print(f' FN = {FN:5},  TP = {TP:5} -> P = {P:5}')\n",
        "    print(f'TNR = {TNR:5.3f}, FPR = {FPR:5.3f}')\n",
        "    print(f'FNR = {FNR:5.3f}, TPR = {TPR:5.3f}')\n",
        "    print(f'ACC = {ACC:6.3f}')\n",
        "\n",
        "    return cm, fpr, tpr, thresholds, auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWc_FiINU4KJ",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plot_training_curves(training_curves_three_layer, phases=['train', 'val', 'test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at this graph, we can see that our model is currently overfitting to the training dataset. As our blue training loss curve is steadily decreasing, we actually see that the validation loss increases. We will tackle the overfitting problem in the next sections!"
      ],
      "metadata": {
        "id": "aALKlL5Aw_uy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgZFioiIU4KK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "res = plot_metrics(three_layer_model, device, dataloaders, phase='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the ROC curve and confusion matrix that was discussed in the module. It looks like our model is incorrectly predicting Young Abalone as Old Abalone at a higher rate then vice-versa."
      ],
      "metadata": {
        "id": "YjnKAjeCx2_O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO0tnP8s2dgI"
      },
      "source": [
        "## Additional Features\n",
        "For the first round of feature selection, we chose to **only use numerical** features. However, we are **losing some critical informatio**n about the abalone by not including their sex in our model. In order to translate this ternary feature from our dataset into the model, we will use what is called a **one hot encoding**. We will now create one feature for each of the different options and assign a value of 1 if the sample is a member of that class, and 0 for the others. Encoding strategies can be simple such as this one hot method or can be much more complex as we will learn later in the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10fjzQGWU4KK"
      },
      "outputs": [],
      "source": [
        "# Insert the new one hot encoding features\n",
        "encoded_df = df.copy(True)\n",
        "encoded_df.insert(1, 'M', 0)\n",
        "encoded_df.insert(1, 'F', 0)\n",
        "encoded_df.insert(1, 'I', 0)\n",
        "encoded_df.loc[(df['Sex'] == 'M'), 'M'] = 1\n",
        "encoded_df.loc[(df['Sex'] == 'F'), 'F'] = 1\n",
        "encoded_df.loc[(df['Sex'] == 'I'), 'I'] = 1\n",
        "encoded_column_names = column_names[:]\n",
        "encoded_column_names.insert(1,\"M\")\n",
        "encoded_column_names.insert(1,\"F\")\n",
        "encoded_column_names.insert(1,\"I\")\n",
        "encoded_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P89lZpnZ5Cba"
      },
      "outputs": [],
      "source": [
        "encoded_feature_columns = encoded_column_names[1:8]\n",
        "print(encoded_feature_columns)\n",
        "label_column = 'Old'\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Ms5ZOI0W06"
      },
      "source": [
        "### Train-Validation-Test Split and Standardization\n",
        "\n",
        "Again we want to perform a Train-Validation-Test split on our dataset. However, we want to make sure that we don't perform standardization on our new features that are one-hot encodings!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'numerical feature columns={numerical_feature_columns}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlGkvtLTcAHb",
        "outputId": "9372529f-e4b0-4bfa-d2b5-29188b5aef6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical feature columns=['Length', 'Diameter', 'Height', 'Whole weight']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWrfoPZd05BZ"
      },
      "outputs": [],
      "source": [
        "# Here we will use the ColumnTransformer to perform datastandardization again. However we are only performing this standardization on the numerical features,\n",
        "# NOT the one hot encoded features, so we specify that in the constructor of the ColumnTransformer.\n",
        "ct = ColumnTransformer([('numerical_features', StandardScaler(), numerical_feature_columns)], remainder='passthrough')\n",
        "\n",
        "encoded_train_dataset, encoded_test_dataset, encoded_val_dataset = train_test_val_split(encoded_df, encoded_feature_columns, label_column, ct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDiYLEKX7mKz"
      },
      "source": [
        "### Model Hyperparameters and Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh48ywI6-0ax"
      },
      "outputs": [],
      "source": [
        "# device config (train our model on GPU if it is available which is much faster)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "input_size = len(encoded_feature_columns) # make sure our input size includes the new features\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 64\n",
        "hidden_size3 = 64\n",
        "num_classes = 2\n",
        "\n",
        "# external training parameters\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bKw6_N2-_-h"
      },
      "outputs": [],
      "source": [
        "three_layer_encoded_model = SimpleClassifier3Layer(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes).to(device)\n",
        "print(three_layer_encoded_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLpVue4k7sDM"
      },
      "source": [
        "### Final Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_x4P5X6-Ylq",
        "outputId": "f59ee323-0ebf-4bdd-8062-680c6970183a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_sizes = {'train': 2506, 'val': 836, 'test': 835}\n"
          ]
        }
      ],
      "source": [
        "# Set up pytorch Datasets and DataLoaders\n",
        "\n",
        "dataloaders = {'train': DataLoader(encoded_train_dataset, batch_size=batch_size),\n",
        "               'val': DataLoader(encoded_val_dataset, batch_size=batch_size),\n",
        "               'test': DataLoader(encoded_test_dataset, batch_size=batch_size)}\n",
        "\n",
        "dataset_sizes = {'train': len(encoded_train_dataset),\n",
        "                 'val': len(encoded_val_dataset),\n",
        "                 'test': len(encoded_test_dataset)}\n",
        "print(f'dataset_sizes = {dataset_sizes}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8YeIhjR-efP"
      },
      "outputs": [],
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(three_layer_encoded_model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "three_layer_encoded_model, training_curves_three_layer_encoded = train_model(three_layer_encoded_model, dataloaders, dataset_sizes,\n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZqikOnu7zOX"
      },
      "source": [
        "By including these new features we actually see little change in our overall performance which indicates that these new features are not offering a significant information gain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWMTZWLb8KIf"
      },
      "source": [
        "### Training Curves and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNiHfJg18l9Z"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(training_curves_three_layer_encoded, phases=['train', 'val', 'test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, it looks like our model is still overfitting to the dataset based on these graphs."
      ],
      "metadata": {
        "id": "_cqJ5x8b6TxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsuYM0en8voG"
      },
      "outputs": [],
      "source": [
        "res = plot_metrics(three_layer_encoded_model, device, dataloaders, phase='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UdLrwahbEF0"
      },
      "source": [
        "## Regularization and Dropout\n",
        "\n",
        "One of the key things we talked about in this module were methods to avoid overfitting. Looking at the training graphs above, we can see that the model significantly overfit to the training dataset. In order to combat this, we have a few tools at our disposal.\n",
        "\n",
        "PyTorch has two of the methods we discussed, Regularization and Dropout built in! We will examine how both of these affects the performance of the model.\n",
        "\n",
        "Standardization 标准化作用于数据，改变数据的分布和尺度。\n",
        "\n",
        "Regularization 正则化作用于模型的参数，通过约束参数来控制模型复杂度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKy0vUu29r0K"
      },
      "source": [
        "### Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYw5YCRQ9K9K",
        "outputId": "4f3cab98-4bf7-47bb-c325-661b28319672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleClassifier3Layer(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=64, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Reset the model\n",
        "three_layer_encoded_l2_model = SimpleClassifier3Layer(input_size, hidden_size1, hidden_size2, hidden_size3,\n",
        "                         num_classes).to(device)\n",
        "print(three_layer_encoded_l2_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyhpMfgBb94D"
      },
      "outputs": [],
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# By adding a weight_decay term to the optimizer, we are including L2 Regularization!\n",
        "optimizer = torch.optim.Adam(three_layer_encoded_l2_model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "three_layer_encoded_l2_model, training_curves_three_layer_encoded_l2 = train_model(three_layer_encoded_l2_model, dataloaders, dataset_sizes,\n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLGfeHwH_aSJ"
      },
      "source": [
        "#### Training Curves and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njHLc4qN_dHZ"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(training_curves_three_layer_encoded_l2, phases=['train', 'val', 'test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the graphs above we do not see the same overfitting behavior to the validation set, however there are still other methods we can use to improve generalization even further."
      ],
      "metadata": {
        "id": "KHa91PNn7BYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg3klkzl_g1Q"
      },
      "outputs": [],
      "source": [
        "res = plot_metrics(three_layer_encoded_l2_model, device, dataloaders, phase='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZrzXg0c9uoY"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8mcxTej-uiX"
      },
      "outputs": [],
      "source": [
        "# device config (train our model on GPU if it is available which is much faster)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "# model architecture\n",
        "input_size = len(encoded_feature_columns)\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 64\n",
        "hidden_size3 = 64\n",
        "num_classes = 2\n",
        "dropout = .25\n",
        "\n",
        "# external training parameters\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5GqsKCR9x6l"
      },
      "outputs": [],
      "source": [
        "# Simple three-hidden-layer classification model with dropout\n",
        "class SimpleClassifier3LayerDropout(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes, dropout):\n",
        "        super(SimpleClassifier3LayerDropout, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout) # dropout rate\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(hidden_size2, hidden_size3),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(hidden_size3, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX1eBlH9--Ga"
      },
      "outputs": [],
      "source": [
        "# Reset the model with dropout\n",
        "three_layer_encoded_l2_dropout_model = SimpleClassifier3LayerDropout(input_size, hidden_size1, hidden_size2, hidden_size3,\n",
        "                         num_classes, dropout).to(device)\n",
        "print(three_layer_encoded_l2_dropout_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQEtWMhf_S5d"
      },
      "outputs": [],
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# By adding a weight_decay term to the optimizer, we are including L2 Regularization!\n",
        "optimizer = torch.optim.Adam(three_layer_encoded_l2_dropout_model.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "three_layer_encoded_l2_dropout_model, training_curves_three_layer_encoded_l2_dropout = train_model(three_layer_encoded_l2_dropout_model, dataloaders, dataset_sizes,\n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, including dropout seemed to only make a little change in our overall accuracy. Considering this is a relatively simple deep model (especially compared to some of the Vision focused nets we will be seeing in the future), this is not entirely unexpected and dropout may not be the best choice for this problem. However let's investigate the training curves below."
      ],
      "metadata": {
        "id": "niKRS6plB0oK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vwmj4mS_wFg"
      },
      "source": [
        "#### Training Curves and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlAXVB9K_0BL"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(training_curves_three_layer_encoded_l2_dropout, phases=['train', 'val', 'test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at these curves, we can see that including dropout improved the generalization of our model and kept us from overfitting!"
      ],
      "metadata": {
        "id": "PR6ZQHdvc_a8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TJEtJEZ_zcI"
      },
      "outputs": [],
      "source": [
        "res = plot_metrics(three_layer_encoded_l2_dropout_model, device, dataloaders, phase='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj4YLMee_gi-"
      },
      "source": [
        "## Regression Model\n",
        "\n",
        "Now instead of just doing a binary classification on whether an abalone is young or old - we want to predict the exact number of rings! We can easily adapt our model to perform this regression task instead!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB63dPX2AZo0"
      },
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5ejpQhL_pKb"
      },
      "outputs": [],
      "source": [
        "regression_label_column = 'Rings' # just switch the label column!\n",
        "# Rings needs to be float as well, otherwise our Loss Function won't work\n",
        "encoded_df[regression_label_column] = encoded_df[regression_label_column].astype('float32')\n",
        "encoded_df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byhGjrLHAgg1"
      },
      "source": [
        "### Train-Validation-Test Split and Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmjBPOI-AoE1"
      },
      "outputs": [],
      "source": [
        "ct = ColumnTransformer([('numerical_features', StandardScaler(), numerical_feature_columns)], remainder='passthrough')\n",
        "\n",
        "regression_train_dataset, regression_test_dataset, regression_val_dataset = train_test_val_split(encoded_df, encoded_feature_columns, regression_label_column, ct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9HJLfwdAsda"
      },
      "source": [
        "### Model Hyperparameters and Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enydwYGrAHlr"
      },
      "outputs": [],
      "source": [
        "# device config (train our model on GPU if it is available which is much faster)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "# model architecture\n",
        "input_size = len(encoded_feature_columns)\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 64\n",
        "hidden_size3 = 64\n",
        "output_neurons = 1\n",
        "dropout = .25\n",
        "\n",
        "# external training parameters\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXJt8d1VhCGw"
      },
      "outputs": [],
      "source": [
        "# Simple three-hidden-layer regression model with dropout\n",
        "class SimpleRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_neurons, dropout):\n",
        "        super(SimpleRegressor, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout) # dropout rate\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(hidden_size2, hidden_size3),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(hidden_size3, output_neurons),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW6TSNg_hj6b"
      },
      "outputs": [],
      "source": [
        "regression_model = SimpleRegressor(input_size, hidden_size1, hidden_size2, hidden_size3, output_neurons, dropout).to(device)\n",
        "print(regression_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcrIz-FtBdTU"
      },
      "source": [
        "### Final Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRIi6ojCAKRM"
      },
      "outputs": [],
      "source": [
        "# Set up pytorch Datasets and DataLoaders\n",
        "\n",
        "dataloaders = {'train': DataLoader(regression_train_dataset, batch_size=batch_size),\n",
        "               'val': DataLoader(regression_val_dataset, batch_size=batch_size),\n",
        "               'test': DataLoader(regression_test_dataset, batch_size=batch_size)}\n",
        "\n",
        "dataset_sizes = {'train': len(regression_train_dataset),\n",
        "                 'val': len(regression_val_dataset),\n",
        "                 'test': len(regression_test_dataset)}\n",
        "print(f'dataset_sizes = {dataset_sizes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1fa2a5dBva5"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "We will augment the training procedure from before to now fit our regression problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XNyz6pNh86x"
      },
      "outputs": [],
      "source": [
        "# From https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "\n",
        "def train_regression_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = np.inf\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Each epoch has a training, validation, and test phase\n",
        "    phases = ['train', 'val', 'test']\n",
        "\n",
        "    # Keep track of how loss and accuracy evolves during training\n",
        "    training_curves = {}\n",
        "    for phase in phases:\n",
        "        training_curves[phase+'_loss'] = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in phases:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, targets in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device) #\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # 对模型输出展平（确保与目标形状一致），直接与真实值计算损失。\n",
        "                    outputs = torch.flatten(model(inputs))\n",
        "                    loss = criterion(outputs, targets)\n",
        "\n",
        "                    # backward + update weights only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            training_curves[phase+'_loss'].append(epoch_loss)\n",
        "\n",
        "            print(f'{phase:5} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            # 基于验证集损失最小保存模型\n",
        "            # deep copy the model if it's the best accuracy (based on validation)\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_epoch = epoch\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Loss: {best_loss:4f} at epoch {best_epoch}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, training_curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB6OEYG8hseH"
      },
      "outputs": [],
      "source": [
        "# loss and optimizer\n",
        "# 使用均方误差损失 衡量预测值与真实值的平方差（聚焦数值误差）。\n",
        "criterion = nn.MSELoss() # MSE Loss instead of CrossEntropy\n",
        "optimizer = torch.optim.Adam(regression_model.parameters(), lr=learning_rate,\n",
        "                             weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "regression_model, training_curves_regression = train_regression_model(regression_model, dataloaders, dataset_sizes,\n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTNZRTbz4c87"
      },
      "outputs": [],
      "source": [
        "plot_training_curves(training_curves_regression, metrics=['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dArSkp6F4c87"
      },
      "outputs": [],
      "source": [
        "predicted_ages, true_ages = regress_predictions(regression_model, device, dataloaders['train'])\n",
        "# You can see that our predictions look sensible compared to the abalone ages!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSI2U1Xa4c87",
        "outputId": "3f74c1fe-524b-4877-d81b-167ef1753aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10.6771],\n",
              "        [ 7.6652],\n",
              "        [11.0545],\n",
              "        ...,\n",
              "        [ 8.3997],\n",
              "        [10.9601],\n",
              "        [ 8.3094]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "predicted_ages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-Q8dg7c4c87"
      },
      "outputs": [],
      "source": [
        "true_ages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0NyaDDw4c87"
      },
      "outputs": [],
      "source": [
        "# Train Results\n",
        "plot_regression(regression_model, device, dataloaders['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCkT20lG4c87"
      },
      "outputs": [],
      "source": [
        "# Validation Results\n",
        "plot_regression(regression_model, device, dataloaders['val'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlIM-RK54c87"
      },
      "outputs": [],
      "source": [
        "# Test Results\n",
        "plot_regression(regression_model, device, dataloaders['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looking Ahead\n",
        "\n",
        "Now that we have built a foundation in PyTorch, we will be able to perform many different machine learning tasks. We will first look to handwritten digit recognition using the MNIST dataset and use that dataset to introduce more complex Deep Neural Network architectures!"
      ],
      "metadata": {
        "id": "doTU0oTMLQhp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}