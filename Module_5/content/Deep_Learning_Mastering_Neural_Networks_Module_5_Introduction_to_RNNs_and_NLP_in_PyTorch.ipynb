{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Mastering Neural Networks - Module 5: Introduction to RNNs and NLP in PyTorch\n",
        "\n",
        "Up until this point, we have yet to apply the power of PyTorch to solving Natural Language Processing (NLP) problems. The field of NLP is an extremely exciting space to apply Deep Learning techniques. Just like in Computer Vision, there are different classes of NLP problems. These include classification, summarization, sequence to sequence mapping, and others. Eventually, we would like to implement a model that solves an extremely popular sequence to sequence mapping problem: Machine Translation. In order to build our NLP toolset though, we will first examine the simpler tasks of word and sentence completion."
      ],
      "metadata": {
        "id": "aZVJ8gUn1RR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Definition\n",
        "\n",
        "Sequence to Sequence Mapping problems are inherently different than the supervised learning tasks we have solved in the past. In Image Classification, there is a singular image with a singular class label, a one-to-one mapping between the input and output. In sequence to sequence mapping, our input consists of many discrete samples, or in our case - characters. Additionally, our output is a sequence of characters.\n"
      ],
      "metadata": {
        "id": "ptJiFULG2AiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "For our dataset, we will use a database of English-Spanish phrase pairs. Eventually we will use this same dataset for our machine translation task but first we will just be examining the English phrases.\n",
        "\n",
        "The PyTorch Tutorial - [NLP From Scratch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) uses an English-French dataset for a translation task and was the inspiration for our use of this dataset. We will perform some similar dataset cleaning and utility functions as they do in their tutorial."
      ],
      "metadata": {
        "id": "cu4mucQioGZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import time, copy\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "yrzZRul42zJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we download and unzip the text file that contains all of our translated phrases\n",
        "!rm spa-eng.zip _about.txt spa.txt\n",
        "!wget https://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoRgk40z3kJk",
        "outputId": "4517b569-d5ad-4391-d5b6-6183226745a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'spa-eng.zip': No such file or directory\n",
            "rm: cannot remove '_about.txt': No such file or directory\n",
            "rm: cannot remove 'spa.txt': No such file or directory\n",
            "--2022-10-19 14:18:40--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5336731 (5.1M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   5.09M  19.8MB/s    in 0.3s    \n",
            "\n",
            "2022-10-19 14:18:41 (19.8 MB/s) - ‘spa-eng.zip’ saved [5336731/5336731]\n",
            "\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n",
            "_about.txt  sample_data  spa-eng.zip  spa.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These helper functions will clean our dataset and ensure the phrases can be used by our neural network. For more information please check out the link in the comments."
      ],
      "metadata": {
        "id": "3KUUMOgBsqIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions combined from PyTorch tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "# This is important because we want all words to be formatted the same similar\n",
        "# to our image normalization\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!'?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def parse_data(filename):\n",
        "    # Read the file and split into lines\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    # Throw out the attribution as it is not a part of the data\n",
        "    pairs = [[pair[0], pair[1]] for pair in pairs]\n",
        "\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "66lR-Qyl8BYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = parse_data(\"spa.txt\")\n",
        "# We only want the english sentences because we aren't translating\n",
        "english_sentences = [pair[0] for pair in pairs]\n",
        "# Shuffle our dataset\n",
        "random.shuffle(english_sentences)\n",
        "print(\"Number of English sentences:\", len(english_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti0WFHKX9-k-",
        "outputId": "0414f508-539e-4596-aaf0-7798de4f8936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of English sentences: 139013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will include all ascii letters and some common characters for our valid character set\n",
        "letters = string.ascii_letters + \" .,;'-\"\n",
        "num_letters = len(letters) + 1 # Need to add 1 for the End Tag"
      ],
      "metadata": {
        "id": "IxR_MgYJ3-dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Completion\n",
        "\n",
        "For our first pass, we will take a sample of the words from our sentences and predict complete words given a subset of characters!"
      ],
      "metadata": {
        "id": "oxGSbaJKzoHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip all of our phrases down to words and throw out '.'\n",
        "words = [word for sentence in english_sentences for word in sentence.split(' ') if word not in ['.', '!', '?'] and len(word) > 0 and '\\'' not in word]\n",
        "\n",
        "# Since we already shuffled our dataset, grab a random sampling of words for our train, val, and test\n",
        "train_words = words[:20000]\n",
        "val_words = words[20000:25000]\n",
        "test_words = words[25000:30000]"
      ],
      "metadata": {
        "id": "BngUGRRE-8P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding\n",
        "\n",
        "Now that we have all of our words in string format, we need to encode them in a representation that the neural network can understand. We will create a one hot encoding vector for each character and concatenate those to represent a specific word for our input training samples.\n",
        "\n",
        "Since we are using a RNN, we will be feeding each of these character vectors in one at a time. The goal of the RNN is to predict the *next* character in the word. This allows the network to generate a word given a single letter or a prefix. Additionally, we need the network to learn when the end of a word is. To achieve this, we will designate a special character for the end of a given sequence (`<EOS>`) that should be predicted by the last character in a word.\n",
        "\n",
        "Therefore, we will designate two tensor sequences. First we create the `input_tensor` which is represented as the concatenation of the one hot encoding vectors for each character. The second tensor sequence, `target_tensor` contains the one hot encoding vector for the target characters corresponding to each character in the input tensor. For example, the `input_tensor`,`target_tensor` character correspondance for the word \"apple\" is:\n",
        "\n",
        "`input_tensor = \"apple\"`\n",
        "\n",
        "`target_tensor = \"pple<EOS>`\n",
        "\n"
      ],
      "metadata": {
        "id": "pszv87RStebQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now make our training samples:\n",
        "\n",
        "# Adapted from https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
        "# One-hot matrix of first to last letters (not including EOS) for input\n",
        "def input_tensor(word):\n",
        "    tensor = torch.zeros(len(word), 1, num_letters)\n",
        "    for idx in range(len(word)):\n",
        "        letter = word[idx]\n",
        "        tensor[idx][0][letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# LongTensor of second letter to end (EOS) for target\n",
        "def target_tensor(word):\n",
        "    tensor = torch.zeros(len(word), 1, num_letters)\n",
        "    for idx in range(1, len(word)):\n",
        "        letter = word[idx]\n",
        "        tensor[idx-1][0][letters.find(letter)] = 1\n",
        "    tensor[len(word)-1][0][num_letters-1] = 1 # EOS\n",
        "    return tensor\n",
        "\n",
        "train_input = [input_tensor(word) for word in train_words]\n",
        "train_target = [target_tensor(word) for word in train_words]\n",
        "val_input = [input_tensor(word) for word in val_words]\n",
        "val_target = [target_tensor(word) for word in val_words]\n",
        "test_input = [input_tensor(word) for word in test_words]\n",
        "test_target = [target_tensor(word) for word in test_words]"
      ],
      "metadata": {
        "id": "3MLDsB9KSYvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at a few word encodings, to see what those look like:\n",
        "for i in range(6):\n",
        "    print(train_words[i], \"[encode as]\", train_input[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqsnvhHEaWCF",
        "outputId": "fc2a563d-6a58-410e-be4b-cb22eb494c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do [encode as] tensor([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "you [encode as] tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "know [encode as] tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "which [encode as] tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "room [encode as] tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "the [encode as] tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's look at the pairing of the input_tensors to the target_tensors\n",
        "def input_tensor_to_word(tensor):\n",
        "    word = \"\"\n",
        "    for i in range(tensor.size(0)):\n",
        "        topv, topi = tensor[i].topk(1)\n",
        "        word += letters[topi[0][0]]\n",
        "    return word\n",
        "\n",
        "def target_tensor_to_word(tensor):\n",
        "    word = \"\"\n",
        "    for i in range(tensor.size(0)):\n",
        "        topv, topi = tensor[i].topk(1)\n",
        "        if topi[0][0] == num_letters-1:\n",
        "            word += \"<EOS>\"\n",
        "            break\n",
        "        word += letters[topi[0][0]]\n",
        "    return word\n",
        "print(\"This code helps visualize which characters represent an input_tensor and its corresponding target_tensor\")\n",
        "examples_to_show = 6\n",
        "count = 1\n",
        "for input, target in zip(train_input, train_target):\n",
        "    print(input_tensor_to_word(input))\n",
        "    print(target_tensor_to_word(target))\n",
        "    count += 1\n",
        "    if count > examples_to_show:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "AnIg_qVDQhy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a41c625-6bc9-47a1-e747-11db75414bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This code helps visualize which characters represent an input_tensor and its corresponding target_tensor\n",
            "do\n",
            "o<EOS>\n",
            "you\n",
            "ou<EOS>\n",
            "know\n",
            "now<EOS>\n",
            "which\n",
            "hich<EOS>\n",
            "room\n",
            "oom<EOS>\n",
            "the\n",
            "he<EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {'train': list(zip(train_input, train_target)),\n",
        "               'val': list(zip(val_input, val_target)),\n",
        "               'test': list(zip(test_input, test_target))}\n",
        "\n",
        "dataset_sizes = {'train': len(train_input),\n",
        "                 'val': len(val_input),\n",
        "                 'test': len(test_input)}\n",
        "print(f'dataset_sizes = {dataset_sizes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SjmOheE6bcn",
        "outputId": "ddd8b577-5073-48f9-91ed-65d755771f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_sizes = {'train': 20000, 'val': 5000, 'test': 5000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Definition\n",
        "\n",
        "The model we create to solve this task will be a basic implementation of a Recurrent Neural Network (RNN). Luckily for us, PyTorch has a great implementation of an RNN module. We will use this built in RNN module and modify our standard `forward` method to account for the hidden state!\n",
        "\n",
        "By using an RNN, our neural network is able to make predictions on sequences of characters. Our forward pass will be slightly modified and include a hidden state that encodes information about the current sequence that the neural network has already seen. In addition to the encoded hidden state, the output of our forward pass will be the predicted probabilities for the next character in the sequence.\n",
        "\n",
        "Our training will be slightly different as the network will be predicting probabilities for the next character in the entire sequence, so we will be keeping track of the loss for an entire sequence before performing back propagation. We will explain this in more depth below.\n"
      ],
      "metadata": {
        "id": "dKtltT2-oUuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers = 1, nonlinearity = 'tanh', dropout = 0)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        output, hidden = self.rnn(input, hidden)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    # This method will give us a hidden state tensor to use for our initial prediction of the sequence\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "metadata": {
        "id": "bYLj4Qvzxldp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use a hidden size of 128 for this network\n",
        "word_rnn = RNN(num_letters, 128, num_letters).to(device)"
      ],
      "metadata": {
        "id": "DoQcZxUF-QBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Training the model will be very similar to classification tasks we have done in the past, however we will now account for the sequence of inputs and predictions. When training, we will calculate the loss for an entire sequence of characters (word) and perform back propagation at the end of a given sequence.\n",
        "\n",
        "We can define the steps for a given sequence below:\n",
        "\n",
        "1. Initialize the hidden state - Each call of `forward` requires a hidden state encoding. We have decided that our initial hidden state encoding for the first character in a sequence is a `zeros` tensor.\n",
        "2. Loop through the `input_tensor` and perform the following steps for each character in the sequence:\n",
        "\n",
        "    a. Call `forward` with the current character's encoding and the current `hidden_state`.\n",
        "\n",
        "    b. `forward` will output a probability distribution of the next predicted character and a new `hidden_state`\n",
        "\n",
        "    c. Calculate the loss of the output probability distribution of the next character with respect to the ground truth next character in the sequence and add it to the running total.\n",
        "\n",
        "    d. Continue to the next character in the input sequence using the latest `hidden_state`\n",
        "3. Once all of the characters in the sequence have been fed through the `forward` function, perform backpropagation given the entire loss of the sequence and redo this process for the next sequence.\n",
        "\n",
        "Note: our training function has been updated to represent this new workflow. Try and walk through the comments in the code to see how we implemented this new process in Python!\n",
        "\n"
      ],
      "metadata": {
        "id": "jUr2jUTXxl6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rnn(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) # keep the best weights stored separately\n",
        "    best_loss = np.inf\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Each epoch has a training, validation, and test phase\n",
        "    phases = ['train', 'val', 'test']\n",
        "\n",
        "    # Keep track of how loss evolves during training\n",
        "    training_curves = {}\n",
        "    for phase in phases:\n",
        "        training_curves[phase+'_loss'] = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in phases:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data\n",
        "            for input_sequence, target_sequence in dataloaders[phase]:\n",
        "                # Now Iterate through each sequence here:\n",
        "\n",
        "                hidden = model.initHidden() # Start with a fresh hidden state\n",
        "\n",
        "                current_input_sequence = input_sequence.to(device)\n",
        "                current_target_sequence = target_sequence.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    loss = 0\n",
        "                    # Make a prediction for each element in the sequence,\n",
        "                    # keeping track of the hidden state along the way\n",
        "                    for i in range(current_input_sequence.size(0)):\n",
        "                        current_hidden = hidden.to(device)\n",
        "                        output, hidden = model(current_input_sequence[i], current_hidden)\n",
        "                        l = criterion(output, current_target_sequence[i])\n",
        "                        loss += l\n",
        "\n",
        "                    # backward + update weights only if in training phase at the end of a sequence\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() / current_input_sequence.size(0)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            training_curves[phase+'_loss'].append(epoch_loss)\n",
        "\n",
        "            print(f'{phase:5} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            # deep copy the model if it's the best loss\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "              best_epoch = epoch\n",
        "              best_loss = epoch_loss\n",
        "              best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Loss: {best_loss:4f} at epoch {best_epoch}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, training_curves"
      ],
      "metadata": {
        "id": "HRR_nA2ofl8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This should look very similar to our previous tasks!\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10 # We are only doing 10 Epochs here to save time, feel free to do longer!"
      ],
      "metadata": {
        "id": "LIGWf9aAFpBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Very similar code to training non recurrent models before!\n",
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification!\n",
        "optimizer = torch.optim.Adam(word_rnn.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "# Train the model. We also will store the results of training to visualize\n",
        "word_rnn, training_curves = train_rnn(word_rnn, dataloaders, dataset_sizes,\n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzqzmUwL8Chj",
        "outputId": "f506c50c-060f-4079-9122-0d9a226d3703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 1.4985\n",
            "val   Loss: 1.3527\n",
            "test  Loss: 1.3650\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 1.2999\n",
            "val   Loss: 1.2613\n",
            "test  Loss: 1.2710\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 1.2216\n",
            "val   Loss: 1.2241\n",
            "test  Loss: 1.2369\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.1778\n",
            "val   Loss: 1.1934\n",
            "test  Loss: 1.2065\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 1.1466\n",
            "val   Loss: 1.1685\n",
            "test  Loss: 1.1812\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 1.1243\n",
            "val   Loss: 1.1588\n",
            "test  Loss: 1.1717\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 1.1059\n",
            "val   Loss: 1.1542\n",
            "test  Loss: 1.1643\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 1.0916\n",
            "val   Loss: 1.1385\n",
            "test  Loss: 1.1477\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 1.0787\n",
            "val   Loss: 1.1307\n",
            "test  Loss: 1.1387\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 1.0678\n",
            "val   Loss: 1.1201\n",
            "test  Loss: 1.1248\n",
            "\n",
            "Training complete in 13m 59s\n",
            "Best val Loss: 1.120111 at epoch 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Results\n",
        "\n",
        "We can visualize our training curve just like in our previous tasks. Here it looks like we are not overfitting after 10 Epochs - we could probably train this net longer!"
      ],
      "metadata": {
        "id": "Diwgt3Fix2zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curves(training_curves,\n",
        "                         phases=['train', 'val', 'test'],\n",
        "                         metrics=['loss']):\n",
        "    epochs = list(range(len(training_curves['train_loss'])))\n",
        "    for metric in metrics:\n",
        "        plt.figure()\n",
        "        plt.title(f'Training curves - {metric}')\n",
        "        for phase in phases:\n",
        "            key = phase+'_'+metric\n",
        "            if key in training_curves:\n",
        "                plt.plot(epochs, training_curves[key])\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(labels=phases)"
      ],
      "metadata": {
        "id": "GEWXj81iCrvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(training_curves, phases=['train', 'val', 'test'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "BgaR32cNCxUC",
        "outputId": "9594ab29-72cb-4142-9683-a25b7f8bc5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xV9f3H8dcne+9FEkjCSiAgYYO4RQVrcaDitlZrteNXbWu1y6pdtvWn/jq0te5iFQSpE1QURCvIDMhIWAmQhOy9x/3+/jiXkGAWWTf35vN8PPLIzblnfM5V3vnme77ne8QYg1JKKefn5ugClFJK9Q8NdKWUchEa6Eop5SI00JVSykVooCullIvQQFdKKRehga4GnIisFpFb+3vd4UxEEkXEiIiHo2tRQ4foOHTVERGpbvOjH9AAtNh//rYx5pXBr0qdICKJQBbgaYxpdmw1aqjQ3+6qQ8aYgBOvRSQbuMMYs/bU9UTEYzgFynA7X+VctMtFnRYROU9EckTkfhHJB14QkVAReUdEikSkzP46vs0260XkDvvrb4jIZyLymH3dLBFZ2Mt1k0Rkg4hUichaEfmbiCztovbLRSRdRCpF5JCILLAvzxaR+W3We+jEftp0bdwuIkeBj+3dQt87Zd87ReQq++sUEflQREpFJFNErm2z3qUistdec66I/Li3/y1OOX6siLxlP+ZBEflWm/dmichW+3kXiMjj9uU+IrJUREpEpFxEtohIdH/UoxxDA131RgwQBiQAd2L9f/SC/edRQB3w1y62nw1kAhHAH4HnRER6se6/gc1AOPAQcHNnBxSRWcDLwH1ACHAOkN3lWbZ3LjABuAR4Fbi+zb4nYp37uyLiD3xory0KuA54yr4OwHNYXVaBwCTg49OooSuvATlALHA18DsRucD+3v8B/2eMCQLGAMvty28FgoGRWJ/hXVj/7ZST0kBXvWEDfmWMaTDG1BljSowxK40xtcaYKuC3WAHYmSPGmH8aY1qAl4ARQGctww7XFZFRwEzgQWNMozHmM+CtLo55O/C8MeZDY4zNGJNrjMk4jXN+yBhTY4ypA1YBaSKSYH/vRuANY0wDcBmQbYx5wRjTbIzZAawErrGv2wRMFJEgY0yZMWb7adTQIREZCcwD7jfG1Btj0oFngVvaHHOsiEQYY6qNMZvaLA8HxhpjWowx24wxlX2tRzmOBrrqjSJjTP2JH0TET0T+ISJHRKQS2ACEiIh7J9vnn3hhjKm1vww4zXVjgdI2ywCOdVHzSOBQF+93p3Xf9l9a72K1vsFqrZ+4SJwAzLZ3YZSLSDlW4MfY318MXAocEZFPRGRuRwcTkT0iUm3/Orub2k58FlVtlh0B4uyvbwfGAxn2bpXL7Mv/BbwPvCYieSLyRxHx7OZYagjTQFe9cerQqB8BycBs+5/159iXd9aN0h+OA2Ei4tdm2cgu1j+G1d3QkRqskTwnxHSwzqnn/CpwvT2QfYB1bY7ziTEmpM1XgDHmbgBjzBZjzOVY3TH/4WT3R/uDGZNq3y7AGPNpF+cFkIf1WQS2WTYKyLXv64Ax5nr7Mf8ArBARf2NMkzHmYWPMROBMrL8ubkE5LQ101R8Csfpey0UkDPjVQB/QGHME2Ao8JCJe9mD9ehebPAfcJiIXioibiMSJSIr9vXTgOhHxFJEZWH3Q3XkPqzX+CLDMGGOzL38HGC8iN9v35ykiM0Vkgr3OG0Uk2BjTBFRidV/1iTHmGPA58Hv7hc4zsFrlJy7s3iQikfYay+2b2UTkfBGZbP9LqhKrC6bP9SjH0UBX/eFJwBcoBjYBawbpuDcCc4ES4DfAMqzx8l9hjNkM3AY8AVQAn2AFMsAvsVrvZcDDWBc0u2TvL38DmN92fXu3x8VY3TF5WF1GfwC87avcDGTbu6busp9Df7geSLQfcxXWNY4Tw0wXAHvEurfg/4Dr7NcCYoAVWGG+D+sz+Vc/1aMcQG8sUi5DRJYBGcaYAf8LQamhSFvoymnZuzLG2LtQFgCXY/VLKzUs6Z2iypnFYHV7hGONwb7bPkxQqWFJu1yUUspFaJeLUkq5CId1uURERJjExERHHV4ppZzStm3bio0xkR2957BAT0xMZOvWrY46vFJKOSUROdLZe9rlopRSLkIDXSmlXIQGulJKuQgdh66UcipNTU3k5ORQX1/f/cpOzMfHh/j4eDw9ez4BZreBLiLPY83CVmiMmdTB++cBb2I93xCseaEf6XEFSil1GnJycggMDCQxMZHOn4vi3IwxlJSUkJOTQ1JSUo+360mXy4tYk/t05VNjTJr9S8NcKTVg6uvrCQ8Pd9kwBxARwsPDT/uvkG4D3RizASjtbWFKKdXfXDnMT+jNOfbXRdG59ofkrhaR1H7aZ4cOFlbxyNt7aWzWaZuVUqqt/gj07UCCMWYK8Be6mO1ORO60P318a1FRUa8OdrS0luf/m8W6zMLeVauUUn1QXl7OU089ddrbXXrppZSXl3e/Yh/0OdCNMZXGmGr76/cATxGJ6GTdZ4wxM4wxMyIjO7xztVvnjIskIsCbldtyel+0Ukr1UmeB3tzc3OV27733HiEhIQNVFtAPgS4iMWLv7BGRWfZ9lvR1v53xcHfjirRY1mUWUlrTOFCHUUqpDj3wwAMcOnSItLQ0Zs6cydlnn82iRYuYOHEiAFdccQXTp08nNTWVZ555pnW7xMREiouLyc7OZsKECXzrW98iNTWViy++mLq6un6prSfDFl8FzgMiRCQH63mRngDGmL9jPX/xbhFpxnqu5HVmgOfkXTw9nmc/y+Kt9Fy+Ma/nQ3qUUq7l4bf3sDevsl/3OTE2iF99vfNLgY8++ii7d+8mPT2d9evX87WvfY3du3e3Di98/vnnCQsLo66ujpkzZ7J48WLCw8Pb7ePAgQO8+uqr/POf/+Taa69l5cqV3HTTTX2uvdtAtz8tvKv3/wr8tc+VnIYJI4KYOCKIlds10JVSjjVr1qx2Y8X//Oc/s2rVKgCOHTvGgQMHvhLoSUlJpKWlATB9+nSys7P7pRanvVN08fR4fv3OXvYXVDE+OtDR5SilHKCrlvRg8ff3b329fv161q5dy8aNG/Hz8+O8887rcCy5t7d362t3d/d+63Jx2rlcLk+LxcNN9OKoUmpQBQYGUlVV1eF7FRUVhIaG4ufnR0ZGBps2bRrU2pw20CMCvDkvOZJVO3JpbtEx6UqpwREeHs68efOYNGkS9913X7v3FixYQHNzMxMmTOCBBx5gzpw5g1qbw54pOmPGDNPXB1ys/vI4d7+ynRdvm8l5yVH9VJlSaijbt28fEyZMcHQZg6KjcxWRbcaYGR2t77QtdIALJkQR7OvJyu25ji5FKaUczqkD3dvDnUVTYvlgTz6V9U2OLkcppRzKqQMdrNEuDc023t113NGlKKWUQzl9oE+JD2ZMpL+OdlFKDXtOH+giwuLp8Ww9UkZ2cY2jy1FKKYdx+kAHuHJqHCLwxnZtpSulhi+XCPQRwb6cNTaCldtzsdkcMwxTKaU6EhAQMGjHcolAB1g8LZ7c8jq+yNKHKymlhienncvlVJekxhDg7cHK7TnMHRPe/QZKKdULDzzwACNHjuS73/0uAA899BAeHh6sW7eOsrIympqa+M1vfsPll18+6LW5TKD7erlz6eQY3t11nEcuT8XPy2VOTSnVmdUPQP6X/bvPmMmw8NFO316yZAn33HNPa6AvX76c999/n//5n/8hKCiI4uJi5syZw6JFiwb92acu0+UCVrdLTWMLa3bnO7oUpZSLmjp1KoWFheTl5bFz505CQ0OJiYnhZz/7GWeccQbz588nNzeXgoKCQa/NpZqxMxPDGBnmy8rtOVw1Ld7R5SilBloXLemBdM0117BixQry8/NZsmQJr7zyCkVFRWzbtg1PT08SExM7nDZ3oLlUC93NTbhqajyfHyohr7x/5hdWSqlTLVmyhNdee40VK1ZwzTXXUFFRQVRUFJ6enqxbt44jR444pC6XCnSwul2MgVU7dMIupdTASE1Npaqqiri4OEaMGMGNN97I1q1bmTx5Mi+//DIpKSkOqcululwARoX7MSsxjJXbcvjOeWMG/aKEUmp4+PLLkxdjIyIi2LhxY4frVVdXD1ZJrtdCB1g8PY7DxTXsOFbu6FKUUmrQuGSgXzp5BD6ebjphl1JqWHHJQA/08eSS1Bje3plHfVOLo8tRSqlB4ZKBDtbF0cr6Zj7aV+joUpRSalC4bKDPGxtBTJAPK3UGRqXUMOGyge7uJlwxNY5P9hdRVNXg6HKUUmrAuWygA1w9PY4Wm+HNdB2TrpTqH+Xl5Tz11FO92vbJJ5+ktra2nys6yaUDfWxUIFNGhrBCR7sopfrJUA50l7ux6FRXT4vjl2/uYU9eBamxwY4uRynl5B544AEOHTpEWloaF110EVFRUSxfvpyGhgauvPJKHn74YWpqarj22mvJycmhpaWFX/7ylxQUFJCXl8f5559PREQE69at6/faXD7Qvz4lll+/s4+V23I10JVyMX/Y/AcySjP6dZ8pYSncP+v+Tt9/9NFH2b17N+np6XzwwQesWLGCzZs3Y4xh0aJFbNiwgaKiImJjY3n33XcBqKioIDg4mMcff5x169YRERHRrzWf4NJdLgAhfl5cOCGKN9NzaWqxObocpZQL+eCDD/jggw+YOnUq06ZNIyMjgwMHDjB58mQ+/PBD7r//fj799FOCgwenMenyLXSwxqSv3p3PJ5lFzJ8Y7ehylFL9pKuW9GAwxvDTn/6Ub3/72195b/v27bz33nv84he/4MILL+TBBx8c8HpcvoUOcG5yJOH+XjomXSnVZ4GBgVRVVQFwySWX8Pzzz7dOwJWbm9v68As/Pz9uuukm7rvvPrZv3/6VbQfCsGihe7q7cXlaHEs3HaG8tpEQPy9Hl6SUclLh4eHMmzePSZMmsXDhQm644Qbmzp0LQEBAAEuXLuXgwYPcd999uLm54enpydNPPw3AnXfeyYIFC4iNjR2Qi6JijOn3nfbEjBkzzNatWwfteHvyKvjanz/j15encvPcxEE7rlKqf+3bt48JEyY4uoxB0dG5isg2Y8yMjtYfFl0uAKmxwaTEBLJiu95kpJRyTcMm0AGunh7PzmPlHCwcvAnnlVJqsAyrQL88LQ53N9GLo0o5OUd1FQ+m3pxjt4EuIs+LSKGI7O5mvZki0iwiV592FYMkMtCbc8dHsmp7Li021/8fQilX5OPjQ0lJiUuHujGGkpISfHx8Tmu7noxyeRH4K/ByZyuIiDvwB+CD0zq6AyyeFs/HGdv5/FAxZ4+LdHQ5SqnTFB8fT05ODkVFRY4uZUD5+PgQHx9/Wtt0G+jGmA0iktjNat8HVgIzT+voDnDhhCiCfDxYuS1HA10pJ+Tp6UlSUpKjyxiS+tyHLiJxwJXA0z1Y904R2SoiWx3129XH052vT4llzZ58quqbHFKDUkoNhP64KPokcL8xptuJUowxzxhjZhhjZkRGOq51vHh6PPVNNlZ/me+wGpRSqr/1R6DPAF4TkWzgauApEbmiH/Y7YKaODGF0hD8rdLSLUsqF9DnQjTFJxphEY0wisAL4jjHmP32ubACJCIunx7M5q5RjpQM32bxSSg2mngxbfBXYCCSLSI6I3C4id4nIXQNf3sC5cmocIuiYdKWUy+jJKJfre7ozY8w3+lTNIIoN8eXMMeG8sT2XH1w4DhFxdElKKdUnw+pO0VMtnhbP0dJatmSXOboUpZTqs2Ed6AsmxeDv5c5KfYi0UsoFDOtA9/PyYOHkEbz75XHqGlscXY5SSvXJsA50sLpdqhua+WCvjklXSjm3YR/os5PCiAvxZYV2uyilnNywD3Q3N2HxtDj+e7CY/Ip6R5ejlFK9NuwDHeCqafHYDKzaoU8zUko5Lw10IDHCnxkJoazcnuPScywrpVybBrrd4unxHCysZldOhaNLUUqpXnG6QLcZG5/mfNrv+/3aGSPw9nDTqQCUUk7L6QL9jQNv8J2PvsPbh97u1/0G+XhycWoMb+3Mo6FZx6QrpZyP0wX65WMvZ3r0dB7Z+AiZpZn9uu/F0+Ior21iXUZhv+5XKaUGg9MFuqebJ4+d+xiBXoHcs+4eKhr6r8/77HGRRAV6s2KbjnZRSjkfpwt0gAjfCB4/73Hya/L5+Wc/x9b9w5J6xN1NuHJqHOszCympbuiXfSql1GBxykAHSItK476Z9/FJzif8Y9c/+m2/i6fH02wzvJme12/7VEqpweB8gV58AJbdBA3VXJ9yPZeNvoyn05/ut5Ev46MDmRwXrKNdlFJOx/kCvSIHMt6F/9yFAA/OfZBxoeN44NMHOFZ1rF8OsXhaHHvyKsnIr+yX/Sml1GBwvkAfcz5c9GvY9zZ8+hi+Hr48ed6TGAw/XP9D6prr+nyIRWlxeLqLzpOulHIqzhfoAHO/C5OvhY9/C5lrGBk0kkfPfpSM0gx+s+k3fb59P8zfi/OTo1i1I4/mlv654KqUUgPNOQNdBBb9GUacAW98C4r2c078Odw95W7eOvQWyzOX9/kQi6fHU1zdwKcHivuhYKWUGnjOGegAnr6w5BVw94LXboD6Cu6achdnx53No1seJb0wvU+7Pz85ilA/T1boxVGllJNw3kAHCBkJ174MZVnwxp24Gfj92b8n2i+aH63/EcV1vW9de3m4cXlaHB/uLaCitqkfi1ZKqYHh3IEOkDgPFjwK+9fA+t8R7B3Mk+c/SUVjBfd9ch/NtuZe73rxtHgam22886WOSVdKDX3OH+gAM++AqTfDhj/B3jdJCUvhwbkPsrVgK09ue7LXu50UF8T46AAd7aKUcgquEegi8LX/hfiZsOpuKNjDojGLWJK8hJf2vsSa7DW93K2weFo824+Wc7ioup+LVkqp/uUagQ7g4Q3X/gu8A62LpLWl3D/zfqZETuHB/z7IwbKDvdrtlVPjcBN4Y7tO2KWUGtpcJ9ABgkbAkqVQmQcrvoknwv+e+7/4evhy7/p7qW48/VZ2VJAPZ4+LZNWOXGw2fTydUmrocq1ABxg5Ey59DA6vg48eIto/msfOfYxjVcf4xX9/0aubjhZPjye3vI5Nh0sGoGCllOofrhfoANNvtS6Ufv4X2PU6M2Nmcu/0e/no6Ec8t/u5097dxROjCfTx0DHpSqkhzTUDHayhjAnz4K3vQV46t0y8hQWJC/jLjr+wMW/jae3Kx9Ody84YwZrd+dQ09H4YpFJKDSTXDXR3T7jmJfALh2U3ITXFPHzmwyQFJfGTDT/hePXx09rd4mnx1Da2sHp3/gAVrJRSfeO6gQ4QEAnXvQI1RfD6N/Bz8+SJ85+gydbEvevvpaGl508lmp4QSmK4n45JV0oNWa4d6ACxU+Hrf4Yjn8H7PycpOInfnvVb9pTs4fdf/L7HuxERrpoWz8bDJeSU1Q5gwUop1TuuH+gAU5bA3O/B5n/AjqVcOOpC7ph8BysPrGTl/pU93s2VU+MAWKVj0pVSQ9DwCHSA+Q/D6PPgnXshZyvfS/sec0bM4Xdf/I49xXt6tIuRYX7MGR3GGzty+zznulJK9bfhE+juHnD1CxA4ApbdhHtNEX8854+E+4Zz7/p7Kasv69FuFk+LJ6u4hu1He7a+UkoNluET6AB+YXDdv6G+ApbdTKiHH0+c9wQldSX8ZMNPaLG1dLuLhZNH4Ovpzopt2u2ilBpaug10EXleRApFZHcn718uIrtEJF1EtorIWf1fZj+KmQRXPAU5m+G9H5MaPpGfz/k5m45v4q/pf+128wBvDxZOiuGdXXnUN3X/C0AppQZLT1roLwILunj/I2CKMSYN+CbwbD/UNbBSr4SzfwTbX4atz3PVuKtYPG4xz375LB8d/ajbzRdPj6eqvpkP9xYMQrFKKdUz3Qa6MWYDUNrF+9Xm5BVCf8A5rhae/3MYdzGs/gkc+Zyfzv4pqeGp/Pyzn5NVkdXlpnNHhxMb7MNKnQpAKTWE9EsfuohcKSIZwLtYrfTO1rvT3i2ztaioqD8O3Xtu7nDVPyE0EZbfgnd1EU+c9wRebl7cu+5eaps6H2vu5iZcM2Mk6zOLWL7l2ODVrJRSXeiXQDfGrDLGpABXAL/uYr1njDEzjDEzIiMj++PQfeMbYl0kbaqH125khHcIfzz3j2RVZvHg5w92OTTxu+eP5dzxkTzwxi7e3XV60wgopdRA6NdRLvbumdEiEtGf+x1Qkclw1TNwPB3evoc5MbP5/tTv8372+7y89+VON/PycOPvN01nekIo9yzbwbrMwkEsWimlvqrPgS4iY0VE7K+nAd6Ac00cnnIpnPcz2PUabHqa2yfdzoWjLuSJbU+wJX9Lp5v5ernz3DdmkhwTyN1Lt7E5q9NLDUopNeB6MmzxVWAjkCwiOSJyu4jcJSJ32VdZDOwWkXTgb8AS44y3UZ5zH6RcBh/8Asn6hN/M+w0jA0fy409+TEFN56NZgnw8eem2WcSF+PLNF7fwZU7FIBatlFIniaOyd8aMGWbr1q0OOXanGqrg2flQXQB3rueQtHD9u9czLnQcL17yIp7unp1uml9Rz9V//5yahmaWf3su46IDB69updSwISLbjDEzOnpveN0p2h3vQOsiqbHBazcyxi+GX8/7NbuKdvGHLX/octOYYB9euWM2Hu5u3PTcFxwr1RkZlVKDSwP9VOFj4OrnoXAvvPldLkm4mFsn3sqyzGW8deitLjdNCPdn6e2zaWi2ceOzX1BQWT9IRSullAZ6x8bOhwt/BXtWwWdPcM/0e5gZM5NHNj5CRmlGl5smxwTy0m2zKKlu4ObnvqCspnGQilZKDXca6J2Z9wOYtBg+egSPQ+v44zl/JNg7mHvW3UNFQ9cXPqeMDOHZW2dypKSWW1/YTFV90yAVrZQazjTQOyMCi/5qTea14nYiait4/LzHKagt4IFPH8BmbF1uPndMOE/fNI29eZXc/tJWnchLKTXgNNC74uUHS16xpgl49XqmBCbxwMwH+Cz3M/6+8+/dbn5BSjSPL0ljS3Ypdy/dRmNz178ElFKqLzTQuxOaANe+BCUHYdVdXDvuahaNWcTTO59mQ86GbjdfNCWW314xmXWZRfxweTotNucboq+Ucg4a6D2RdA5c8jvIfBf59E/8cs4vSQlL4d519/LIxkc4Wnm0y81vmD2Kn12awju7jvPzVV/q4+uUUgPCw9EFOI3Z34bjO2H97/GJmcxTFz7F39L/xn8O/ocV+1cwP2E+35z0TSZFTOpw8zvPGUNVfTN/+fgggT4e/OzSCdhnTFBKqX6hd4qejqZ6eGEhFO+HOz6CqBSKaot4Zd8rLM9cTlVTFbNiZnHbpNuYFzvvK4FtjOHht/fy4ufZ/Oii8Xz/wnEOOhGllLPq6k5RDfTTVZELz5wH3gHwrXXWFLxAdWM1K/av4F97/0VhXSHjQ8dz26TbuCTxEjzdTk4ZYLMZ7luxi5Xbc/jV1ydy27wkB52IUsoZaaD3t6Ob4MXLYNQcWPQXCDsZyk0tTbxz+B1e3PMihysOE+sfyy2pt3Dl2Cvx8/QDoLnFxvf+vYM1e/L509VncM2MkY46E6WUk9FAHwg7XoF37gXTAmk3wNk/tkbE2NmMjQ05G3h+9/PsKNxBsHcw1yVfxw0TbiDMJ4yG5hbueGkr/z1YzN9umMbCySMceDJKKWehgT5QKo/DZ0/AthfAGJh6k/Xw6ZD2Le4dhTt4YfcLrDu2Dm93b64YewW3pt5KuPcIbn5uM7tyynn21pmcO34IPMVJKTWkaaAPtIpc+Oxx2PaS9fP0W+GsH0JwXLvVDpcf5sU9L/L24bexGRsXJ1zMNeNu5qHXKzhcXM3S22czIzHMASeglHIWGuiDpfwYfPq/sONfIG4w/TY4614Iat+dUlhbyNK9S1m+fzk1TTVMj5rFwQMzqShJ5NU75zIpLthBJ6CUGuo00Adb2RH49DFI/ze4ecCMb8K8eyAwut1qVY1VvL7/dZbuXUpRXRFuTfFIxfksu/FOkmNCHFS8Umoo00B3lNIs2PAY7HwV3L1g5u3WLI4BUe1Wa2xp5J3D7/DMzufJrTmCNIdzV9pt3DblWnw9fB1UvFJqKNJAd7SSQ7DhT7BrGXj4wMw7rGD3j2i3ms3Y+Neu93hs09/B5whBXsHcNOFGrk+5nhAfbbErpTTQh47ig7Dhj/Dl6+DhC7PvhDP/B/zaXwjddqSUm19Zhl/kBhq8duPr4cuVY6/kltRbiAuI62TnSqnhQAN9qCnaD5/8AXavBC9/mH0XzP1uu2D//GAx33hxC2Piqjlj4g4+OLIag+GSxEu4bdJtpISlOPAElFKOooE+VBXus4J9zyrwDoI5d8Oc77ROJ/Dh3gLuWrqNWYlhPLpkFK/v/zev73+d2uZazow9k29O+iazYmbpJF9KDSMa6ENdwR5Y/yjsewu8g2Hud6xw9wnmPztyuXd5OhemRPH0TdOpa6lmeeZylu5dSkl9CanhqdyaeivzR83H092z+2MppZyaBrqzyP/SCvaMd8AnGOZ+H2Z/m6XpZfziP7u5PC2WJ65Nw81NaGhp4O1Db/Pinhc5UnmEcJ9wFo9fzDXjryHGP8bRZ6KUGiAa6M4mL90K9v2rwTcUzvw+/2y4mN+uPcqNs0fxmysmtXaz2IyN/+b+l2WZy9iQswER4dz4c7ku+TrmxM7BTfQZJkq5Eg10Z5W73Qr2A++DXzjrI67n7v3TuOXciTywIOUrfee51bms2L+CNw68QWl9KaMCR3Ft8rVcMfYKgr317lOlXIEGurPL2Qrrfw8H11LtEcKTdV8j8vy7+fb8yR2u3tjSyIdHPmRZ5jJ2FO7A292bhUkLWZK8pNMnKimlnIMGuqs4thmz7nfI4XUUmWCyU+5k5tU/As/O7ybNLM1keeZy3j78NnXNdaSGp7IkeQkLkhboXahKOSENdBfTnPVfDiz7ORPqd1DnHYnvuffA1But/vZOVDdW8/bht1mWsYxDFYcI8griirFXcG3ytSQEJXS6nVJqaNFAd0H1TS386ZnnuKjgBea47bXuPD3jGmtagRFTOt3OGMPWgq0sy1zGR0c+otk0c2bsmVybfC3nxp+Lh5s+N1ypoUwD3UXVNDRz83NfUH8snZ9FfMaZda8pbhIAABypSURBVB/j1lwP8bNg1rdg4uXg4d3p9kW1Rbxx4A1e3/86BbUFRPtFc834a1g8fjERvhGdbqeUchwNdBfW0NzCs59m8dePD+Jvqnl8/B7OKn8Tt9JD4BcB026xpu8N6fy5pc22Zj7J+YRlGcvYeHwjHuLB/IT5LElewvTo6XonqlJDiAb6MJBXXsdv39vHu7uOkxDqwxMzK5ha8Dqyf421wvgFVnfM6PPBrfOx6Ucqj7A8czmrDq6iqrGKsSFjWZK8hMtGX0aAV8AgnY1SqjMa6MPI5weL+dVbezhQWM35yZE8fF4wow4vsx6PV1sMYWOsednTbujyImpdcx1rstbwWuZr7C3Zi5+HH5eNvowlKUsYHzp+EM9IKdWWBvow09Ri46XPs3ly7QEam21865wkvnv2SPwOvgdb/gnHvrAuok6+2upr7+IiKsDu4t28lvEaa7LX0NDSwLSoaSxJXsL8hPl4uXsN0lkppUADfdgqrKrn0dUZvLE9lxHBPvziaxO5dHIMkv8lbHnWmpe9qda6iDrzDki9osuLqBUNFfzn4H9Ynrmco1VHCfMJY/G4xVw9/mpiA2IH8cyUGr400Ie5rdmlPPjmHvYer+TMMeE8vCiVcdGBUFduPfd0y7PQ7iLqbRAyqtP92YyNTXmbeC3zNT7J+QSAc+LO4cpxV3JW3FnaaldqAPUp0EXkeeAyoNAY85X7xkXkRuB+QIAq4G5jzM7uitJAH1wtNsO/Nx/lsfczqWlo5htnJvKD+eMI9PEEmw2y1sOW5yDzPWuDHl5EPV59nBUHVrBy/0pK6ksI9AzkglEXsDBpIbNGzMLTTaf0Vao/9TXQzwGqgZc7CfQzgX3GmDIRWQg8ZIyZ3V1RGuiOUVrTyJ/ez+S1LUeJCPDmpwtTuHJq3MmhieXHYNsLp30RtcnWxObjm1mdtZqPjn5EdVM1od6hXJRwEQuSFjAtahrubu6DdJZKua4+d7mISCLwTkeBfsp6ocBuY0y3D77UQHesXTnlPPjmHtKPlTMjIZSHL08lNbbNjIzNDbD3rV5dRG1saeSz3M9Yk7WG9TnrqWuuI9I3kksSL2FB0gLOiDhDx7Yr1UuDGeg/BlKMMXd08v6dwJ0Ao0aNmn7kyJFuj60Gjs1mWLEthz+syaCstpEbZyfwo4vHE+J3Sh/48V2nXESdCTO/1e1FVIDaplo25G5gTdYaPs35lEZbI3EBcVySeAkLkxaSHJqs4a7UaRiUQBeR84GngLOMMSXd7VNb6ENHRW0TT6zdz8sbswn29eQnC1K4dsZI3N1OCdq6ctj5qhXuJQfBL7zNnaidX0Q9oaqxinXH1rE6azWb8jbRbJpJDEpkQdICFiYuZHTI6IE5QaVcyIAHuoicAawCFhpj9vekKA30oWff8Up+9eYeNmeXckZ8MA8vSmXqqA76zTu6iDruEph2M4yd322rHaCsvoy1R9eyJmsNW/K3YDCMDx3PwqSFXJJ4CSMDO5+qQKnhbEADXURGAR8DtxhjPu9pURroQ5Mxhrd25vHbd/dRWNXAtTPi+cmCFCICOgnpUy+iegdBytcg9UprhIxH90MYi2qL+ODIB6zJWkN6UToAkyMmsyBxARcnXqzPSFWqjb6OcnkVOA+IAAqAXwGeAMaYv4vIs8Bi4ESHeHNnB2tLA31oq25o5i8fHeC5z7Lw9XLnRxeN56Y5CXi4dzKEsaUJDn8Ce1ZBxttQXwE+ITDhMivck84F9+6HMOZV5/F+9vuszlrNvtJ9AEyLmsbCpIVclHAR4b7h/XmaSjkdvbFI9drBwmoeemsPnx0sJiUmkIcXpTJ7dDeh2twIhz62h/u70FgFvmEw4esw6SpIOAvcu593/UjlEdZkrWF11moOVRzCTdyYHTObhUkLuWDUBfqcVDUsaaCrPjHG8P6efH79zj5yy+u4Ii2Wn146geggn+43bqqHQx/B7jcgczU01YB/JExYZLXcE86EHoxPP1B2gNVZq1mTvYZjVcfwcPNgXuw8FiQt4PyR5+Pv6d8PZ6rU0KeBrvpFXWMLT68/yN83HMbTTfjB/HF848wkvDw6v5O0naY6OPCB1XLf/741BDIg2noQR+pVMHJ2l3elgvXLZW/J3tZwL6gtwNvdm3Piz2Fh0kLOjjsbH48e/KJRyklpoKt+daSkhl+/s5e1+woZE+nPQ4tSOXtc5OntpLHGCvU9b8CBD6G5HgJjrbHtqVdaY927GZ9uMzbSC9NZk72G97Pfp7S+FD8PP8aGjiUuII74gHjiAuKIC4wjLiCOGP8YnYpAOT0NdDUgPs4o4OG393KkpJaFk2K4f0EKiRG96PpoqILMNVbL/eCH0NIIwSOtlvukqyB2Wrfh3mxrZmvBVtYeWUt2RTY51Tnk1+TTYlpa13ETN6L9oq2Qtwd9a+gHxBHpF4mb9PCvDaUcRANdDZj6phae+yyLv3x8gIZmG+eNj+TWMxM5Z1wkbqfemNSjHVZYfe2737AurNqaICTBarWnXmlNO9DDO0ubbc0U1BaQW5VLbvUpX1W5FNYVtlvfy82L2IBY4gLiWr+3Df0Q7xC9q1U5nAa6GnCFlfW88sVR/r35KEVVDSSG+3Hz3ESumRFPkE8vuznqyqxRMntWweH1YGuGsNH2cL8KolN7HO4daWhpIK86rzXgc6tzyanOaQ39ioaKduv7efi1dt+0bdnHBsQSHxivF2bVoNBAV4OmsdnG6t3HeenzbLYfLcfPy50rp8Zxy9xEkmMCe7/j2lLY95YV7lkbwNggYvzJlnvUhP47CbvqxuoOW/YnQr+uua7d+iHeIe26c6L9oonwjSDKL4pI30gi/SLxdu/+LlqluqKBrhziy5wKXt6YzZs782hstjF3dDi3npnA/AnRnd+g1BPVRSfDPfszwEDkhJPhHjGuTy33njDGUNZQ9tWWvf3nvJo8mm3NX9kuyCuoXcC3/R7lF9X6Wh8Sojqjga4cqrSmkWVbjrF00xFyy+uIDfbhxjkJXDdzJOGdTSnQU1UFsPdNK9yPbgQMeAVCaCKEJti/279CEqxJxDwHflijzdioaKigsLaQoroiimqLKKorsn6uLaK4rpjCukKKa4tpNl8N/mDv4NaQP7WV33a5Bv/wo4GuhoQWm2HtvgJe3pjNfw+W4OXhxtfPiOXWMxM4Iz6k7weozLP63IsPQFm29VV+xBoS2VZgbMdhH5pojYvvZix8f7IZG+UN5RTVWmFfXFfc4S+B4rridiN2TgjxDiHSL5Io3zbB36blH+sfS6TfaQ4pVUOaBroacg4UVPHyxiOs3J5DbWMLaSND+MaZiSycHIO3Rz8+2chmg5rCkwFfdqR92FfmAW3+DXj4WK34jsI+NAG8+3AdoA9sxkZZfVm7gD/R2j8R/oV1hZTUlXwl+GP8Y5gSOYW0yDTSotJIDkvW8fhOTANdDVmV9U2s3JbDyxuPkFVcQ0SAFzfMGsUNsxOICR6EOz6b6qHiWJvAzz4Z9qXZ1jw0bfmFdxL2iRAU16M5agZSi62Fsoay1qA/WnmUXUW7SC9K53jNcQB83H2YGD6RtKg00iLTmBI1hTCfMIfWrXpOA10NeTab4bODxbz0eTYfZxbiLsIlk2K4dW4iMxNDHTP+2xhr6GRZ1ldb9mXZ1tTBbVvDbh4QHG8P+lHWhGQ+weATZM086RNsfXkHnXzt6TvgF3BPyK/JZ2fRTuurcCd7S/e2XrgdFTiKtKg0pkROYUrkFMaGjNVnwA5RGujKqRwtqeVfm7JZtuUYlfXNpMQEcuuZiVyRFoev1xAKmZZmqMzpPOzryqwbo7ri5mkP/A7CvqOvU9/3Cuh1n399cz17S/ays2gn6YXppBelU1pfCoC/pz9nRJzBlCirq2Zy5GSCvIJ6dRzVvzTQlVOqa2zhzfRcXvw8m4z8KoJ8PFgycyQ3z0lkVLifo8vrnjHWBdn6CvtXpf17ufW9obLNe23frzj5flNtNweRU34hnPILwD8CYtOs6RP8uu5WMcaQU5VDelF6a8gfKD+AzdgQhDEhY6y+eHtXTUJQgt456wAa6MqpGWPYkl3GSxuzWbM7H5sxXJAcxa1nJnLW2IjeTTHgLJobOwj+ig5+IXTwy+DE9xPCxkDcdIifYX2Pmdzt4wJrmmr4svjL1hb8rsJdVDVZ1xVCvENaA35K5BQmRUzC18N3ID8NhQa6ciH5FfX8+4sj/HvzUYqrGxkd4c/NcxNYPL0PUwy4svoKyEuH3K2Qux1ytkJ1vvWem6cV6icCPm6GNbVCF104NmMjqyKrNeB3Fu0kqyILAHdxJzksuXU0zZTIKYzwH6Gt+H6mga5cTkNzC6u/zOeljdnsOFqOv5c7V02L55a5CYyLdszQQqdgjDVUM3cr5G6DnG2Qt8N68AhY3TRx008GfNx0COh6HHt5fTm7ineRXmgF/JfFX7ZOixDlG8WUqClMDJ9IrH8ssQHWV4RvhM5s2Usa6Mql7cop56XPj/D2LmuKgdTYIOZPiOaiidGkxgZpC7E7thYoyrAHvL0lX7jHmi8HrBE7bQN+xBTw6vwaRrOtmf1l+09ebC1MJ68mr906nm6exPjHEOsfy4iAEV/5rnPXd04DXQ0LJdUNrNyewwd7Cth2tAxjICbIhwsnRDF/YjRzR4fj4zmERskMZY01cHynPeC3WV8Vx6z3xB2iJ54M+PgZ1kRpXQxzrG2qJa86j7yaPI5XHyevJq/dz0V1Re3WdxM3In0jiQ2IZYT/iHbfT4T+cO2v10BXw05xdQPrMgr5aF8hGw4UUdvYgp+XO2ePi2D+hGguSInq+zwyw01Vwclwz90KuTvgxBTDXoHWaJq2F12DYnu868aWRvJr8r8a+NV5HK85TkFNwVfmvAnzCftK2LeGfkCsyw6z1EBXw1p9UwsbD5fw0b4C1u4tJL+yHhGYNiqU+ROimT8hirFRAdo1c7psNig52Cbgt0H+7pNj7wNjIW6aFfDRk62ADxph3WR1mp91i62Forqidq363Opcjtccbw39hpaGdtsEeAac7MaxB31iUCIpYSnE+Mc47X9vDXSl7Iwx7MmrZO2+AtbuK2B3rjWsLyHczx7u0cxIDMWzL9P7DmdN9ZD/ZZuLrlutO23b8vC1h3ssBI6wQj4ozv7aviwg+rSmUTDGUFpf2i7gc6tzW1v7x6uPtw63BGs2y5TQFJLDkkkJSyElLIXE4ESn6LfXQFeqE8cr6vhoXyFr9xXw+cESGltsBPl4cH5KFPMnRHNucqQOh+yrmhIo3g9VeVB5HKqOQ2Wu/bV92al31IqbFeonQr41/E98j7N+EXj1/ClRlY2VHC4/TEZpBhmlGWSWZnKg/EBry97LzYuxoWNbAz4lLIXxoeOH3JOoNNCV6oGahmY+PVDM2n0FfJxRSGlNIx5uwuzRYa2t95FhTnCHqrOx2aC2pE3g51lDK9sGfmXeyf76tryD7S38WKuLJ2hE+8APjLUmVOtkbH2zrZnsimwyyqyA31e6j4zSjNbHDwrCqKBRJIcmtwv6CN8Ih3XZaKArdZpabIb0Y2V8uNdqvR8srAYgOTqwddRMWnyIa9+lOtQ01pwS+Hn21n6b19UFJ4dbnuDmeTLcQ0Zad8yGj7G+hyV9ZUoEYwwFtQXtWvL7SveRW53buk6YT1i7gE8OSyYhMGFQJjTTQFeqj7KLa1i7r4CP9hWyObuUFpshIsCbC1IimT8hmrPGReDn5dipcxXWhGk1hZ0HftkR+/DLNrnnG3pKyI+G8NHWa9+TD16pbKxkf+l+Mssy2Veyj8yyTA6WH2ydsdLXw5dxoeNa++YnhE1gbOjYfh9eqYGuVD+qqG1i/f5C1u4rZH1mIVX1zXh7uDFvrDUk8sIJUUQHDcJc7qp3muqtGTFLD0PpISg5ZH0vzYKKHNqFvV+4FfCtgT/aHvhjwCeYppYmDlUcateSzyzNpLrJ+ovOTdxaR9acaMmnhKX0af55DXSlBkhTi40tWaV8aB81c6zUuuX9jPhgzhwTwZzRYcxIDCPAW1vvTqGpzgr7kkOnBP5h60JuW34RHbbqTWgSuc1V7bpsMsoyyK/Jb930tkm38cPpP+xViRroSg0CYwwHCqv5cG8B6zIK2ZlTTlOLwd1NmBQXzJzRYcwZHc6MhFACdeSM82mqs1rxp7bqSw5Z/fpt+Ud+pVVfFhhFJk1kVh8lJSyF2SNm96oMDXSlHKCusYXtR8vYdLiETYdLSD9mBbybwOS4YOaMDrcCPlED3uk11tjD/pRWfelhq+++rYBomPtdmPeDXh1KA12pIaCusYUdrQFfSvqxchpbbLgJ9hZ8eGsXjY59dyGNNVawt+3GGXMBTFrcq91poCs1BNU3nWjBl1ot+KMnAz411uqimZ0UzsykMIJ9NeCVRQNdKSdQ39TCjqPlrV00O46V09hsQwRSY4OYkxTO7NHhzEoMI9hPA3640kBXygnVN7WQfuxkwG8/ejLgJ44IYs7ocGYnWa14DfjhQwNdKRdQ39TCzmPlrV0024+W0WAP+Akx9oAfHcbspDBC/LwcXa4aIBroSrmghuYWdh6raG3BbztyMuBTYoLsrfcwJscHExfi67TTxar2+hToIvI8cBlQaIyZ1MH7KcALwDTg58aYx3pSlAa6Uv2robmFXTkVbDpUwqYsK+Drm6x5TYJ9PZkUF8Sk2GBS44JJjQ0iKdxf56JxQn0N9HOAauDlTgI9CkgArgDKNNCVGhoam23syatgT14le/Iq2J1bSWZ+FY0tVsj7e7kzMTaI1Fgr4CfFBTM2KkDngh/iugr0bu9HNsZsEJHELt4vBApF5Gu9rlAp1e+8PNyYOiqUqaNCW5c1tdg4UFDN7rwK9uZVsju3guVbj1Hb2NK6TUpMIKmxwa0t+uSYQH0Wq5MY1AkmRORO4E6AUaNGDeahlVKAp7sbE2ODmBh78nmbLTZDVnFNa2t+d24F7+7K49XNRwFwdxPGRQW0a8lPjA3S+WmGoEH9L2KMeQZ4Bqwul8E8tlKqY+5uwtioAMZGBXB5WhxgzUuTU1bX2lWzO6+CT/YXsXJ7Tut2SRH+rQE/yR72of46usaR9FesUuorRISRYX6MDPNjwaQRrcsLK+vZbQ/5PXkV7Dhazju7Ts5VEhfiS6q9X35SnBX2UYHeOsJmkGigK6V6LCrIhwuCfLggJbp1WVlNI3uPW101u/Mq2ZNbwYf7Cjgx3iIiwJuJsUGkxASSHB1IckwgY6MCtF9+APRklMurwHlABFAA/ArwBDDG/F1EYoCtQBBgwxoRM9EYU9nVfnWUi1Kuq7qhmX32kN+TV8nevEoOFla3jrBxE0iM8LeHfBDJMYGkxAQyKsxPh1J2Q28sUko5XHOLjeySGjLyq9ifX0VGfhWZBVUcLa1tbc37erozLjqgtSWfEmOFfWSgt2OLH0L6NGxRKaX6g4e7G2OjAhkbFQhnnFxe29jM/oJqMvMrycyvJrOgknWZhby+7eQF2HB/L5JjAhkfbbXkT7z215E27einoZRyKD8vD9JGhpA2MqTd8uLqBjLtLfn9+VVkFFSxbMsx6ppaWtcZFebXLuRTYgJJivDHY5jeHKWBrpQakiICvIkY6828sRGty2w2w7GyWqu7xt5lk5lfxbrMQlpsVr+Nl7sbY6ICSDmlRT8i2MflR9tooCulnIabm5AQ7k9CuD+XpMa0Lq9vauFQUbUV8vag33S4hFU7Tj7YOcjHg7FRASRFBDA60p+kCOsrMdwfXy/XGHGjga6Ucno+nu72O1mD2y2vqG2yt+Iryciv4nBRDf89WNzuBimA2GAfklpDPoDREf4kRvgTH+rrVHPbaKArpVxWsJ8ns5LCmJUU1m55TUMz2SU1ZBXXkFVkfT9cXMNb6XlU1je3rufhJowK82ttzZ8I/dERAUQHDb0bpjTQlVLDjr+3R4ctemMMZbVNZBVXc9ge9Ce+/nuouHU6YrCGWJ4I+dH2rpsTrx31gBENdKWUshMRwvy9CPMPY3pC+1a9zWbIr6xvbc1bLftq9uRWsGZ3futFWYBQP8+T3TeD2F+vga6UUj3g5ibEhvgSG+LbbuQNWHPPHyurbe2+ySqxAr+z/vpvnpXEHWeP7vcaNdCVUqqPvDzcGBMZwJjIgK+811F//UDd+aqBrpRSA6iz/vqB4DzjcZRSSnVJA10ppVyEBrpSSrkIDXSllHIRGuhKKeUiNNCVUspFaKArpZSL0EBXSikX4bBniopIEXCkl5tHAMX9WI6z08+jPf08TtLPoj1X+DwSjDGRHb3hsEDvCxHZ2tlDUocj/Tza08/jJP0s2nP1z0O7XJRSykVooCullItw1kB/xtEFDDH6ebSnn8dJ+lm059Kfh1P2oSullPoqZ22hK6WUOoUGulJKuQinC3QRWSAimSJyUEQecHQ9jiQiI0VknYjsFZE9IvIDR9fkaCLiLiI7ROQdR9fiaCISIiIrRCRDRPaJyFxH1+QoInKv/d/IbhF5VUR8HF3TQHCqQBcRd+BvwEJgInC9iEx0bFUO1Qz8yBgzEZgDfHeYfx4APwD2ObqIIeL/gDXGmBRgCsP0cxGROOB/gBnGmEmAO3CdY6saGE4V6MAs4KAx5rAxphF4DbjcwTU5jDHmuDFmu/11FdY/2DjHVuU4IhIPfA141tG1OJqIBAPnAM8BGGMajTHljq3KoTwAXxHxAPyAPAfXMyCcLdDjgGNtfs5hGAdYWyKSCEwFvnBsJQ71JPATwOboQoaAJKAIeMHeBfWsiPg7uihHMMbkAo8BR4HjQIUx5gPHVjUwnC3QVQdEJABYCdxjjKl0dD2OICKXAYXGmG2OrmWI8ACmAU8bY6YCNcCwvOYkIqFYf8knAbGAv4jc5NiqBoazBXouMLLNz/H2ZcOWiHhihfkrxpg3HF2PA80DFolINlZX3AUistSxJTlUDpBjjDnxF9sKrIAfjuYDWcaYImNME/AGcKaDaxoQzhboW4BxIpIkIl5YFzbecnBNDiMigtVHus8Y87ij63EkY8xPjTHxxphErP8vPjbGuGQrrCeMMfnAMRFJti+6ENjrwJIc6SgwR0T87P9mLsRFLxB7OLqA02GMaRaR7wHvY12pft4Ys8fBZTnSPOBm4EsRSbcv+5kx5j0H1qSGju8Dr9gbP4eB2xxcj0MYY74QkRXAdqyRYTtw0SkA9NZ/pZRyEc7W5aKUUqoTGuhKKeUiNNCVUspFaKArpZSL0EBXSikXoYGuVC+IyHk6o6MaajTQlVLKRWigK5cmIjeJyGYRSReRf9jnS68WkSfs82N/JCKR9nXTRGSTiOwSkVX2OUAQkbEislZEdorIdhEZY999QJv5xl+x34WolMNooCuXJSITgCXAPGNMGtAC3Aj4A1uNManAJ8Cv7Ju8DNxvjDkD+LLN8leAvxljpmDNAXLcvnwqcA/W3Pyjse7cVcphnOrWf6VO04XAdGCLvfHsCxRiTa+7zL7OUuAN+/zhIcaYT+zLXwJeF5FAIM4YswrAGFMPYN/fZmNMjv3ndCAR+GzgT0upjmmgK1cmwEvGmJ+2Wyjyy1PW6+38Fw1tXreg/56Ug2mXi3JlHwFXi0gUgIiEiUgC1v/3V9vXuQH4zBhTAZSJyNn25TcDn9ifBJUjIlfY9+EtIn6DehZK9ZC2KJTLMsbsFZFfAB+IiBvQBHwX62EPs+zvFWL1swPcCvzdHthtZye8GfiHiDxi38c1g3gaSvWYzraohh0RqTbGBDi6DqX6m3a5KKWUi9AWulJKuQhtoSullIvQQFdKKRehga6UUi5CA10ppVyEBrpSSrmI/wcNsNXjc7laOgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Words\n",
        "\n",
        "Below we have included a function that runs the network on a specified input sequence. This could be one letter or part of word! We have included some examples below, see what words the network will predict for you!\n",
        "\n",
        "The results of this network will be heavily determined by the random sample of sentences that was given at the beginning. The more a certain word appears in our training data, the more likely it is to be predicted. This is very similar to the word suggestion features you see on your smartphone keyboard!"
      ],
      "metadata": {
        "id": "Hjpv3rrAyUST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, input_letters, max_length = 50):\n",
        "    output_word = input_letters\n",
        "    tensor = input_tensor(input_letters)\n",
        "    hidden = model.initHidden()\n",
        "    current_input_sequence = tensor.to(device)\n",
        "    input = None\n",
        "\n",
        "    for i in range(current_input_sequence.size(0)):\n",
        "        current_hidden = hidden.to(device)\n",
        "        output, hidden = model(current_input_sequence[i], current_hidden)\n",
        "\n",
        "    topv, topi = output.topk(1)\n",
        "    topi = topi[0][0]\n",
        "    if topi == num_letters - 1:\n",
        "        # print(\"Most likely word was our initial letters, grab the second most likely\")\n",
        "        topv, topi = output.topk(2)\n",
        "        topi = topi[0][1]\n",
        "    letter = letters[topi]\n",
        "    output_word += letter\n",
        "    input = input_tensor(letter)\n",
        "\n",
        "    for i in range(len(input_letters), max_length):\n",
        "        current_hidden = hidden.to(device)\n",
        "        current_input = input[0].to(device)\n",
        "        output, hidden = model(current_input, current_hidden)\n",
        "        topv, topi = output.topk(1)\n",
        "        topi = topi[0][0]\n",
        "        if topi == num_letters - 1:\n",
        "            # print(\"Hit the EOS\")\n",
        "            break\n",
        "        letter = letters[topi]\n",
        "        output_word += letter\n",
        "        input = input_tensor(letter)\n",
        "    return output_word"
      ],
      "metadata": {
        "id": "h35LR8siENSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(word_rnn, \"ha\"))\n",
        "print(predict(word_rnn, \"al\"))\n",
        "print(predict(word_rnn, \"c\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbYyrIeVnLsE",
        "outputId": "0ca93d4b-c211-46f1-9939-e727922f55ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n",
            "all\n",
            "can\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(word_rnn, \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7adK27eaODk",
        "outputId": "46a4bcb8-bb66-406f-e9ec-b59c08ec3ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Sentence Completion\n",
        "\n",
        "Now we will turn to the task of sentence completion! This will be relatively straightforward since we will still be working with one hot encodings of sequences of characters. There are other more sophisticated ways to encode our training samples that we will discuss later on."
      ],
      "metadata": {
        "id": "LTTaTWhPLMTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = english_sentences[:1000]\n",
        "val_sentences = english_sentences[1000:2000]\n",
        "test_sentences = english_sentences[2000:3000]"
      ],
      "metadata": {
        "id": "hFSKAkfB3Mm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding\n",
        "\n",
        "Our encoding here is super simple. We will use the exact same function as before, just operating on an entire sentence instead of a singular word!"
      ],
      "metadata": {
        "id": "mRM0QT3Q0Wvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_sentence = [input_tensor(sentence) for sentence in train_sentences]\n",
        "train_target_sentence = [target_tensor(sentence) for sentence in train_sentences]\n",
        "val_input_sentence = [input_tensor(sentence) for sentence in val_sentences]\n",
        "val_target_sentence = [target_tensor(sentence) for sentence in val_sentences]\n",
        "test_input_sentence = [input_tensor(sentence) for sentence in test_sentences]\n",
        "test_target_sentence = [target_tensor(sentence) for sentence in test_sentences]"
      ],
      "metadata": {
        "id": "aRNqiQSw3K79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders_sentences = {'train': list(zip(train_input_sentence, train_target_sentence)),\n",
        "               'val': list(zip(val_input_sentence, val_target_sentence)),\n",
        "               'test': list(zip(test_input_sentence, test_target_sentence))}\n",
        "\n",
        "dataset_sizes_sentences = {'train': len(train_input_sentence),\n",
        "                 'val': len(train_input_sentence),\n",
        "                 'test': len(train_input_sentence)}\n",
        "print(f'dataset_sizes = {dataset_sizes_sentences}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaJtJZJL4KYF",
        "outputId": "4a33d24a-2add-44bb-e43e-5b63f76dff22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_sizes = {'train': 1000, 'val': 1000, 'test': 1000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Definition\n",
        "\n",
        "Additionally, we can use the exact same architecture! We will just use a larger hidden layer since our sequences will be longer."
      ],
      "metadata": {
        "id": "qAwNSS5M0qhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigger rnn\n",
        "sentences_rnn = RNN(num_letters, 256, num_letters).to(device)"
      ],
      "metadata": {
        "id": "Dz9h0YlQ4fhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Here we will train for another 10 Epochs as it takes a very long time to train recurrent models. Increasing the number of epochs should increase the performance of the model."
      ],
      "metadata": {
        "id": "LDqLqS1k0sUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This should look very similar to our previous tasks!\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10 # We are only doing 10 Epochs here to save time, feel free to do longer!"
      ],
      "metadata": {
        "id": "QZhztvmL1KLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for classification!\n",
        "optimizer = torch.optim.Adam(sentences_rnn.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "# Train the model. We also will store the results of training to visualize\n",
        "sentences_rnn, training_curves_sentences = train_rnn(sentences_rnn, dataloaders_sentences, dataset_sizes,\n",
        "                                     criterion, optimizer, scheduler, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doZ1fwqN4r-m",
        "outputId": "6860a04f-1902-4d91-b0b8-8ad83dac5ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 0.1212\n",
            "val   Loss: 0.4222\n",
            "test  Loss: 0.4225\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 0.1024\n",
            "val   Loss: 0.4019\n",
            "test  Loss: 0.4031\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 0.0973\n",
            "val   Loss: 0.3902\n",
            "test  Loss: 0.3916\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 0.0936\n",
            "val   Loss: 0.3799\n",
            "test  Loss: 0.3814\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "train Loss: 0.0904\n",
            "val   Loss: 0.3738\n",
            "test  Loss: 0.3752\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "train Loss: 0.0875\n",
            "val   Loss: 0.3711\n",
            "test  Loss: 0.3727\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "train Loss: 0.0848\n",
            "val   Loss: 0.3679\n",
            "test  Loss: 0.3694\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "train Loss: 0.0822\n",
            "val   Loss: 0.3654\n",
            "test  Loss: 0.3667\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "train Loss: 0.0796\n",
            "val   Loss: 0.3634\n",
            "test  Loss: 0.3651\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "train Loss: 0.0772\n",
            "val   Loss: 0.3605\n",
            "test  Loss: 0.3626\n",
            "\n",
            "Training complete in 6m 24s\n",
            "Best val Loss: 0.360483 at epoch 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Results"
      ],
      "metadata": {
        "id": "xYQTMMVX03v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(training_curves_sentences, phases=['train', 'val', 'test'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "w-FABBjpC6JV",
        "outputId": "097a983a-5779-44f7-b749-e81d3c118f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Sentences\n",
        "\n",
        "We can use the exact same function as before with our new model in order to generate sentences. Try some of your own examples below!"
      ],
      "metadata": {
        "id": "DU6G9V4505v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(sentences_rnn, \"I ate a \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82X4oC3S7aHo",
        "outputId": "204e77cb-e0a3-4851-cf29-2d0654d17868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ate a los .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(sentences_rnn, \"What is\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIyXR6cMQzDP",
        "outputId": "39ab5c0c-5977-4c12-91fa-e2a85d57ae7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is my for the shere .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(sentences_rnn, \"My name is\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifIHKZgxQ3mt",
        "outputId": "bda5ed6c-33d4-4761-e374-87922323d077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is the reand .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(sentences_rnn, \"Ha\"))"
      ],
      "metadata": {
        "id": "y0QadAaXQ8C_",
        "outputId": "7328d70d-a669-4798-94fe-d329ab329eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hat you den't le the thing this .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict(sentences_rnn, \"A\"))"
      ],
      "metadata": {
        "id": "WwGefkBg4NLx",
        "outputId": "5ee7d2ef-5175-4eb3-8b79-98557a6c8336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ao mand mary .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "In this notebook we have seen how to apply our powerful deep neural networks to different sequence based prediction tasks. We can see from the sentence completion results that our network does not produce super intelligent results. In future notebooks we will examine ways to improve our models and tackling more challenging tasks such as machine translation.\n",
        "\n",
        "Some of the methods to increase performance are: Larger datasets, increased training time, more intelligent word level embeddings (word2vec), and more sophisticated deep neural networks (LSTMs and Transformers)."
      ],
      "metadata": {
        "id": "Mr3BkX8z1WNl"
      }
    }
  ]
}